<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Optimization Lecture 3</title>
  <meta name="description" content="Convex OptimizationGradient DescentGradient Descent Algorithm">
  
  <meta name="author" content="YeonjeeJung">
  <meta name="copyright" content="&copy; YeonjeeJung 2022">
  

  <!-- External libraries -->
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/styles/monokai-sublime.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/css/lightbox.css">

  <!-- Favicon and other icons (made with http://www.favicon-generator.org/) -->
  <link rel="shortcut icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/icons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/icons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/icons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/icons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/icons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/icons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/icons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/icons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/icons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/icons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
  <link rel="manifest" href="/assets/icons/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/icons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">

  

  
  <!-- Twitter: card tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Optimization Lecture 3">
  <meta name="twitter:description" content="Convex OptimizationGradient DescentGradient Descent Algorithm">
  <meta name="twitter:image" content="https://yeonjeejung.github.io/assets/logo.png">
  <meta name="twitter:url" content="https://yeonjeejung.github.io">
  

  

  <!-- Site styles -->
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://yeonjeejung.github.io/lecture/optimization/2019/09/09/OptLecture3.html">
  <link rel="alternate" type="application/rss+xml" title="YeonjeeJung's Blog" href="https://yeonjeejung.github.io/feed.xml" />
</head>

  
    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$'] ],
    processEscapes: true,
  }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

  
  <body>

    <header class="navigation" role="banner">
  <div class="navigation-wrapper">
    <a href="/" class="logo">
      
      <img src="/assets/logo.png" alt="YeonjeeJung's Blog">
      
    </a>
    <a href="javascript:void(0)" class="navigation-menu-button" id="js-mobile-menu">
      <i class="fa fa-bars"></i>
    </a>
    <nav role="navigation">
      <ul id="js-navigation-menu" class="navigation-menu show">
        <li class="nav-link"><a href="/about">About</a>
        <li class="nav-link"><a href="/category/thesis">Thesis</a>
        <li class="nav-link"><a href="/category/lecture">Lecture</a>
          <ul>
            <a href="/category/computervision">CV</a>
            <a href="/category/optimization">Optimization</a>
          </ul>
        </li>
      </ul>
    </nav>
  </div>
</header>


    <div class="page-content">
        <div class="post">

<div class="post-header-container " >
  <div class="scrim ">
    <header class="post-header">
      <h1 class="title">Optimization Lecture 3</h1>
      <p class="info">by <strong>Yeonjee Jung</strong></p>
    </header>
  </div>
</div>

<div class="wrapper">

 <span class="page-divider">
  <span class="one"></span>
  <span class="two"></span>
</span>
 

<section class="post-meta">
  <div class="post-date">September 9, 2019</div>
  <div class="post-categories">
  in 
    
    <a href="/category/lecture">Lecture</a>, 
    
  
    
    <a href="/category/optimization">Optimization</a>
    
  
  </div>
</section>

<article class="post-content">
  <h1 id="convex-optimization">Convex Optimization</h1>
<h2 id="gradient-descent">Gradient Descent</h2>
<h3 id="gradient-descent-algorithm">Gradient Descent Algorithm</h3>

<script type="math/tex; mode=display">x_t \leftarrow x_t - \gamma\triangledown f(x_t)</script>

<p>이거나, $\gamma$를 $t$의 함수로 표현하여</p>

<script type="math/tex; mode=display">x_t \leftarrow x_t - \gamma_t\triangledown f(x_t)</script>

<p>로 업데이트 하여 최적점을 찾아가는 방법을 gradient descent 알고리즘이라고 한다. 이 때 $\gamma_t$는 $\frac{1}{t}$, $\frac{1}{\sqrt{t}}$, $\frac{1}{\log{t}}$, $\cos{\theta t}$가 주로 쓰인다.</p>

<h3 id="convergence-rate">Convergence Rate</h3>

<p>Convergence rate은</p>

<script type="math/tex; mode=display">f(x_t)-f(x^* )\le\epsilon</script>

<p>을 만족하는 $t$를 찾기까지의 시행 수를 Big O 표기법으로 나타내는 것이다.</p>

<h2 id="strong-convex">Strong Convex</h2>
<h3 id="alpha-strong-convex">$\alpha$-Strong Convex</h3>

<script type="math/tex; mode=display">f(x)-f(y)\le\triangledown f(x)^T(x-y)-\frac{\alpha}{2}\|x-y\|^2, \forall x,y</script>

<p>를 만족할 때의 $\alpha$를 앞에 붙여 $\alpha$-strong convex라고 한다. 만약 $f$가 미분불가능하면 subgradient로 $\alpha$-strongly convexity를 정할 수 있다. 다르게는</p>

<script type="math/tex; mode=display">f(y)\ge f(x)+\triangledown f(x)^T(y-x)+\frac{\alpha}{2}\|x-y\|^2</script>

<p>로 쓸 수 있다.</p>

<h2 id="smooth">Smooth</h2>
<h3 id="beta-smooth">$\beta$-Smooth</h3>
<p>$\triangledown f$가 $\beta$-Lipschitz라고 하면</p>

<script type="math/tex; mode=display">\|\triangledown f(x)-\triangledown f(y)\|\le\beta\|x-y\|</script>

<p>를 항상 만족하는데, 이때의 $\beta$를 앞에 붙여 $f$ 를 $\beta$-smooth라고 한다.</p>

<script type="math/tex; mode=display">f(x)-f(y) \ge \triangledown f(x)^T(x-y)-\frac{\beta}{2}\|x-y\|^2, \forall x,y</script>

<p>라고도 쓸 수 있고, 또는</p>

<script type="math/tex; mode=display">f(y) \le f(x)+\triangledown f(x)^T(y-x)+\frac{\beta}{2}\|x-y\|^2</script>

<p>라고도 쓸 수 있다.</p>

<p>$\beta$-smooth 함수에서 gradient descent 알고리즘을 사용해 최적값을 찾을 때에는 <script type="math/tex">\|\triangledown f(x)-\triangledown f(x^* )\|\le\beta \|x-x^* \|</script>이고, $\triangledown f(x^* )=0$이기 때문에 learning rate가 $\gamma = \frac{1}{\beta}$인 것이 좋다. 하지만 보통의 convex optimization에서는 $\beta$를 모르기 때문에 learning rate를 정하는 것이 어렵다.</p>

<p>어떤 함수가 $\alpha$-strong이고 $\beta$-smooth이면 항상 $\alpha \le \beta$가 성립한다. 또한, convex optimization에서 $\alpha = \beta$일 때 가장 쉽게 해를 찾을 수 있다. 항상 $0 \le \frac{\alpha}{\beta} \le 1$ 이므로, $\frac{\alpha}{\beta}$가 1에 가까울수록 해를 찾기가 쉽다.</p>

<h2 id="second-order-characterization-of-convexity">Second-order Characterization of convexity</h2>

<p>$f$가 두 번 미분가능하면 $\forall x\in\mathbf{dom}{(f)}$에서 항상 $\triangledown^2f(x)$가 존재한다. 그리고</p>

<script type="math/tex; mode=display">f \text{ is convex }\Leftrightarrow \triangledown^2 f(x) \text{ is positive semidefinite }\forall x\in\mathbf{dom}{(f)}</script>

<p>가 항상 성립한다.</p>

<h3 id="def-positive-definite">(Def) Positive Definite</h3>

<p>$A\in\mathbb{R}^{d\times d}$는</p>

<script type="math/tex; mode=display">x^TAx \gt 0, \forall x\in \mathbb{R}^d</script>

<p>를 만족할 때 positive definite라고 한다.</p>

<script type="math/tex; mode=display">x^TAx\ge 0 , \forall x\in \mathbb{R}^d</script>

<p>를 만족하면 positive semidefinite이라고 한다.</p>

<h2 id="hessian-smooth-strong-convexity">Hessian, Smooth, Strong convexity</h2>

<script type="math/tex; mode=display">% <![CDATA[
f :\alpha\text{-strongly convex & } \beta\text{-smooth } \Leftrightarrow \alpha I \le \triangledown^2f(x)\le\beta I %]]></script>

<p>를 항상 만족하는데, 행렬 $A, B$에 대하여 $A\le B$는 $B-A$가 positive semidefinite임을 뜻한다. 또한, 이는 $\triangledown^2 f(x)$의 eigenvalue들이 $\alpha$보다 크고 $\beta$보다 작음을 뜻한다.</p>

<p>hessian은 이러한 성질 때문에 step size를 정하기에 매우 중요한데, 이를 second-order method라고 한다. 하지만 계산량이 많고 저장할 숫자도 많으므로 딥러닝에서는 first-order method만 사용한다.</p>

<h3 id="proof-rightarrow">(Proof) $\Rightarrow$</h3>

<p>우선 $\alpha$-strongly convex이기 때문에</p>

<p><script type="math/tex">f(y)-f(x)\le\triangledown f(y)^T(y-x)-\frac{\alpha}{2}\|x-y\|^2</script>, <script type="math/tex">f(x)-f(y)\le\triangledown f(x)^T(x-y)-\frac{\alpha}{2}\|x-y\|^2</script></p>

<p>가 항상 성립한다. 마찬가지로 $\beta$-smooth이기 때문에</p>

<p><script type="math/tex">f(y)-f(x)\ge\triangledown f(y)^T(y-x)-\frac{\beta}{2}\|x-y\|^2</script>, <script type="math/tex">f(x)-f(y)\ge\triangledown f(x)^T(x-y)-\frac{\beta}{2}\|x-y\|^2</script></p>

<p>가 항상 성립한다. $\alpha$에 대한 두 식을 합치면</p>

<script type="math/tex; mode=display">0\le(\triangledown f(x)-\triangledown f(y))^T(x-y)-\alpha\|x-y\|^2\cdots(1)</script>

<p>를 얻게 되고, 마찬가지로 $\beta$에 대한 식을 합치면</p>

<script type="math/tex; mode=display">0\ge(\triangledown f(x)-\triangledown f(y))^T(x-y)-\beta\|x-y\|^2\cdots(2)</script>

<p>를 얻게 된다. $x=y+ht$룰 대입하면 ($h$는 벡터, $t$는 스칼라)</p>

<script type="math/tex; mode=display">(\triangledown f(x)-\triangledown f(y))^T(x-y) = \frac{\triangledown f(y+ht)-\triangledown f(y)}{t}ht^2</script>

<p>를 얻을 수 있는데, 여기에 극한을 취하면</p>

<script type="math/tex; mode=display">\lim_{t\rightarrow 0}{\frac{\triangledown f(y+ht)-\triangledown f(y)}{t}}ht^2=h^T\triangledown^2f(y)ht^2</script>

<p>을 얻을 수 있다. 이를 $(1)$과 $(2)$에 대입하면</p>

<script type="math/tex; mode=display">\alpha t^2\|h\|^2\le h^T\triangledown^2f(y)t^2h\le \beta t^2\|h\|^2</script>

<p>를 얻게 되고,</p>

<script type="math/tex; mode=display">h^T\alpha h\le h^T\triangledown^2 f(y)h\le h^T\beta h, \forall h</script>

<p>를 얻을 수 있다.</p>

<script type="math/tex; mode=display">0\le h^T(\triangledown^2 f(y)-\alpha)h, 0\le h^T(\beta-\triangledown^2 f(y))h , \forall h</script>

<p>이므로,</p>

<script type="math/tex; mode=display">\alpha I \le \triangledown^2 f(y)\le \beta I</script>

<p>와 동치이다.</p>

<h3 id="proof-leftarrow">(Proof) $\Leftarrow$</h3>

<p>Taylor Thm을 이용하면</p>

<script type="math/tex; mode=display">f(y) = f(x) + \triangledown f(x)^T(y-x) + \frac{1}{2}(y-x)^T\triangledown^2f(c)(y-x), c=x+(y-x)t, \forall t\in[0, 1]</script>

<p>라고 할 수 있는데, 이 때</p>

<script type="math/tex; mode=display">\frac{1}{2}(y-x)T\triangledown f(c)^T(y-x)\ge\frac{1}{2}k\|y-x\|^2</script>

<p>를 만족하는 $k$를 $\alpha$라고 하고,</p>

<script type="math/tex; mode=display">\frac{1}{2}(y-x)T\triangledown f(c)^T(y-x)\le\frac{1}{2}k\|y-x\|^2</script>

<p>를 만족하는 $k$를 $\beta$라고 하면 $f$는 $\alpha$-strongly convex이고 $\beta$-smooth이다.</p>

<h2 id="vanila-analysis">Vanila Analysis</h2>
<p>만약 $f$가 strongly convex도 아니고, smooth하지도 않지만 $L$-Lipschitz continuous하다고 했을 때, $f(x)-f(x^* )$를 gradient descent를 이용해 어떻게 bound하는지에 대해 알아보자. 여기서는 <script type="math/tex">2a^Tb = \|a\|^2+\|b\|^2-\|a-b\|^2 \cdots(* )</script>임을 이용할 것이다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}f(x_t)-f(x^* )&\le\triangledown f(x_t)^T(x_t-x^* )\\
&=\frac{1}{\gamma}(x_t-x_{t+1})^T(x_t-x^* )(x_{t+1} = x_t-\gamma\triangledown f(x_t)\\
&=\frac{1}{2\gamma}(\|x_t-x_{t+1}\|^2+\|x_t-x^* \|^2-\|x_{t+1}-x^* \|^2)(\text{by }(* ))\\
&=\frac{\gamma}{2}\|\triangledown f(x_t)\|^2+\frac{1}{2\gamma}(\|x_t-x^* \| -\|x_{t+1}-x^* \|^2)\end{align} %]]></script>

<p>이들을 모든 $t$에 대해 다 더하면,</p>

<script type="math/tex; mode=display">\sum_{t=0}^{T-1}\left(f(x_t)-f(x^* )\right)\le\frac{1}{2\gamma}(\|x_0-x^* \|^2-\|x_T-x^* \|^2) + \sum_{t=0}^{T-1}\frac{\gamma}{2}\|\triangledown f(x_t)\|^2</script>

<p>가 된다. $f$가 Lipschitz continuous라고 했으므로 항상 <script type="math/tex">\|\triangledown f(x_t)\|^2 \le L^2</script>이다. <script type="math/tex">\bar{x} = \frac{x_0+\cdots+x_{T-1}}{T}</script>라고 하면, <script type="math/tex">f(\bar{x})\le\frac{1}{T}\sum_{t=0}^{T-1}f(x_t)</script> 임이 성립하고,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}f(\bar{x})-f(x^* )&\le\frac{1}{T}\sum_{t=0}^{T-1}\left(f(x_t)-f(x^* )\right)\\
&\le \frac{1}{2\gamma T}\|x_0-x^* \|^2 + \frac{\gamma}{2}L^2 (\text{Lipschitz continuous})\\
&=\frac{1}{2\gamma T}R^2 + \frac{\gamma}{2}L^2 (\text{Let }R = x_0-x^* ) \end{align} %]]></script>

<p>가 된다. 따라서 여기에 적합한 learning rate 를 rough하게 계산해보면 $\gamma = \frac{R}{L\sqrt{T}}$가 적합하다. (마지막 식을 미분해서 0 되는 $\gamma$ 찾음)</p>

<p>이를 대입해 보면</p>

<script type="math/tex; mode=display">f(\bar{x})-f(x^* )\le\frac{RL}{\sqrt{T}}</script>

<p>가 성립한다.</p>

<h3 id="l-lipschitz-continuous">$L$-Lipschitz Continuous</h3>

<script type="math/tex; mode=display">\|f(x)-f(y)\| \le L\|x-y\|, \forall x,y</script>

<p>를 만족할 때 $L$-Lipschitz continuous라고 하고, $f$가 미분가능하면 <script type="math/tex">\|\triangledown f(x)\|\le L</script>을 만족할 때 Lipschitz continuous라고 한다.</p>

<p>이 때 $\gamma = \frac{1}{L}$로 잡으면 diverge는 방지할 수 있지만 fast converge는 보장하지 못한다.</p>

</article>





<section class="rss">
  <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
</section>

<!-- <section class="share">
  <span>Share: </span>
  
    
    
    
    
    
    
    
    
  
</section> -->




<section class="disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_shortname = 'true';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



</div>
</div>

    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h3 class="footer-heading">YeonjeeJung's Blog</h3>

    <div class="site-navigation">

      <p><strong>Site Map</strong></p>
      <ul class="pages">

        <li class="nav-link"><a href="/about">About</a>
        <li class="nav-link"><a href="/category/note">Note</a>
        <li class="nav-link"><a href="/category/thesis">Thesis</a>
        <li class="nav-link"><a href="/category/lecture">Lecture</a>

      </ul>
    </div>

    <div class="site-contact">

      <p><strong>Contact</strong></p>
      <ul class="social-media-list">
        <li>
          <a href="mailto:gotnwlsl@gmail.com">
            <i class="fa fa-envelope-o"></i>
            <span class="username">gotnwlsl@gmail.com</span>
          </a>
        </li>

        
          
          <li>
            <a href="https://github.com/YeonjeeJung" title="Fork me on GitHub">
              <i class="fa fa-github"></i>
              <span class="username">YeonjeeJung</span>
            </a>
          </li>
          
        

      </ul>
    </div>

    <div class="site-signature">
      <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
      <p class="text"></p>
    </div>

  </div>

</footer>

<!-- Scripts -->
<script src="//code.jquery.com/jquery-1.11.2.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/js/lightbox.min.js"></script>

<script type="text/javascript">
$(document).ready(function() {
  // Default syntax highlighting
  hljs.initHighlightingOnLoad();

  // Header
  var menuToggle = $('#js-mobile-menu').unbind();
  $('#js-navigation-menu').removeClass("show");
  menuToggle.on('click', function(e) {
    e.preventDefault();
    $('#js-navigation-menu').slideToggle(function(){
      if($('#js-navigation-menu').is(':hidden')) {
        $('#js-navigation-menu').removeAttr('style');
      }
    });
  });
});

</script>






  </body>

</html>
