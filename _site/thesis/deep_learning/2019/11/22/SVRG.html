<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Accelerating Stochastic Gradient Descent using Predictive Variance Reduction</title>
  <meta name="description" content="">
  
  <meta name="author" content="YeonjeeJung">
  <meta name="copyright" content="&copy; YeonjeeJung 2023">
  

  <!-- External libraries -->
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/styles/monokai-sublime.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/css/lightbox.css">

  <!-- Favicon and other icons (made with http://www.favicon-generator.org/) -->
  <link rel="shortcut icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/icons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/icons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/icons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/icons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/icons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/icons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/icons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/icons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/icons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/icons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
  <link rel="manifest" href="/assets/icons/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/icons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">

  

  
  <!-- Twitter: card tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Accelerating Stochastic Gradient Descent using Predictive Variance Reduction">
  <meta name="twitter:description" content="">
  <meta name="twitter:image" content="https://yeonjeejung.github.io/assets/logo.png">
  <meta name="twitter:url" content="https://yeonjeejung.github.io">
  

  

  <!-- Site styles -->
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://yeonjeejung.github.io/thesis/deep_learning/2019/11/22/SVRG.html">
  <link rel="alternate" type="application/rss+xml" title="YeonjeeJung's Blog" href="https://yeonjeejung.github.io/feed.xml" />
</head>

  
    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$'] ],
    processEscapes: true,
  }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

  
  <body>

    <header class="navigation" role="banner">
  <div class="navigation-wrapper">
    <a href="/" class="logo">
      
      <img src="/assets/logo.png" alt="YeonjeeJung's Blog">
      
    </a>
    <a href="javascript:void(0)" class="navigation-menu-button" id="js-mobile-menu">
      <i class="fa fa-bars"></i>
    </a>
    <nav role="navigation">
      <ul id="js-navigation-menu" class="navigation-menu show">
        <li class="nav-link"><a href="/about">About</a>
        <li class="nav-link"><a href="/category/thesis">Thesis</a>
          <ul>
            <a href="/category/Deep_Learning">Deep Learning</a>
            <a href="/category/Continual_Learning">Continual Learning</a>
            <a href="/category/Multi_modal_Learning">Multi-modal Learning</a>
          </ul>
        <li class="nav-link"><a href="/category/lecture">Lecture</a>
          <ul>
            <a href="/category/computervision">CV</a>
            <a href="/category/optimization">Optimization</a>
          </ul>
        </li>
      </ul>
    </nav>
  </div>
</header>


    <div class="page-content">
        <div class="post">

<div class="post-header-container " >
  <div class="scrim ">
    <header class="post-header">
      <h1 class="title">Accelerating Stochastic Gradient Descent using Predictive Variance Reduction</h1>
      <p class="info">by <strong>Yeonjee Jung</strong></p>
    </header>
  </div>
</div>

<div class="wrapper">

 <span class="page-divider">
  <span class="one"></span>
  <span class="two"></span>
</span>
 

<section class="post-meta">
  <div class="post-date">November 22, 2019</div>
  <div class="post-categories">
  in 
    
    <a href="/category/thesis">Thesis</a>, 
    
  
    
    <a href="/category/deep_learning">Deep_Learning</a>
    
  
  </div>
</section>

<article class="post-content">
  <hr />

<h2 id="abstract">[Abstract]</h2>

<p>SGD의 본질적인 분산 때문에 느리고 점진적으로 수렴하는 단점을 해결하고자 이 논문에서는 SVRG라고 불리는 explicit한 분산 감소 방법을 제안한다. smooth하고 strongly convex한 함수에 대해서는 SDCA, SAG와 같은 수렴속도를 증명했다. SDCA, SAG와의 차이점은 gradient를 저장할 필요가 없다.</p>

<h2 id="1-introduction">[1] Introduction</h2>

<p>머신러닝에서 푸는 문제는 주로</p>

<script type="math/tex; mode=display">\min P(w), P(w):=\frac{1}{n}\sum_{i=1}^n\psi_i(w)</script>

<p>의 형태이다. (주로 $\psi$는 loss function이다) 주로 이를 풀기 위해 SGD를 사용하는데, 일반적인 형태는</p>

<script type="math/tex; mode=display">w^{(t)}=w^{(t-1)}-\eta_tg_t(w^{(t-1)}, \xi_t)</script>

<p>이다. SGD는 분산 때문에 computation과 convergence 사이에 trade-off가 있는데, 이를 개선하기 위해 여러 연구가 있었다. SDCA와 SAG가 대표적인데, 이들은 이전의 gradient를 저장하기 때문에 딥러닝같은 복잡한 문제에는 적합하지 않다. 이 논문에서 제안하는 방법은</p>

<ol>
  <li>gradient를 저장하지 않아도 되고, 따라서 SDCA나 SAG가 적용하지 못하는 복잡한 문제에 적용할 수 있다.</li>
  <li>SGD의 분산 감소와 직접적으로 연결될 수 있는 증명을 제시한다.</li>
  <li>nonconvex 최적화에도 적용될 수 있다.</li>
</ol>

<p>는 장점을 갖고 있다.</p>

<h2 id="2-stochastic-variance-reduced-gradient">[2] Stochastic Variance Reduced Gradient</h2>

<p>이 논문에서 제안하는 방법에서는 지정된 시간마다 $\tilde{w}$를 저장한다. 그리고 평균 gradient $\tilde{\mu}$ 또한 저장한다. 이 논문에서 제안하는 update rule은 다음과 같다.</p>

<script type="math/tex; mode=display">w^{(t)}=w^{(t-1)}-\eta_t(\triangledown\psi_i(w^{(t-1)})-\triangledown\psi_{i_t}(\tilde{w})+\tilde{\mu})</script>

<p>SGD와는 다르게 이 방법에 있는 learning rate $\eta_t$는 decay 할 필요가 없고, 따라서 수렴 속도를 높여준다. gradient 계산이 끝나고 나서 $\tilde{w_s}$를 정하는 옵션에는 두 가지가 있는데,</p>

<ol>
  <li>$\tilde{w_s}=w_m$으로 정하는 방법</li>
  <li>$t\in {0, \cdots , m-1}$에서 랜덤으로 선택된 $t$에 대해 $\tilde{w_s}=w_t$으로 선택하는 방법</li>
</ol>

<p>이 있다. practical하게는 첫번째 방법을 쓰지만, 이 논문에서는 분석에서 두번째 방법을 사용했다.</p>

<h2 id="3-analysis">[3] Analysis</h2>

<p>우선 모든 $\psi_i(w)$는 convex하고 $P(w)$는 strongly convex하다고 한다.</p>

<h3 id="thm-1">[Thm 1]</h3>

<p>$w_* =\arg\min_w P(w)$라고 하자. $m$이 충분히 커서</p>

<script type="math/tex; mode=display">\alpha = \frac{1}{\gamma\eta(1-2L\eta)m}+\frac{2L\eta}{1-2L\eta}\lt 1</script>

<p>이면, 평균에서 다음과 같은 기하 수렴을 갖게 된다.</p>

<script type="math/tex; mode=display">\mathbb{E}P(\tilde{w_s})\le \mathbb{E}P(w_* )+\alpha^s[P(\tilde{w_* })-P(w_* )]</script>

<p>이 수렴도를 SAG나 SDCA와 비교해보자. condition number가 $\frac{L}{\gamma}=n$인 사례를 생각해 보면 batch gradient descent에서는 정확도 $\epsilon$을 얻기 위해 반복 당 $n\ln(\frac{1}{\epsilon})$의 복잡도가 나온다. 그러나 SVRG는 $n\ln(\frac{1}{\epsilon})$의 복잡도가 나온다. 이 복잡도들은 SAG나 SDCA와 비슷한 수준이며, SVRG가 더 직관적이므로 더 낫다.</p>

<p>SVRG는 smooth하지만 strongly convex 하지 않은 경우에도 적용될 수 있는데, 수렴도는 $O(\frac{1}{T})$로, SGD의 수렴도인 $O(\frac{1}{\sqrt{T}})$보다 개선되었다. 인공신경망같은 nonconvex 문제에 SVRG를 이용하려면 local minimum에 가까운 $\tilde{w_0}$에서부터 시작하는 것이 좋다. 그러면 그 이후에 빠르게 수렴할 수 있다.</p>

<h2 id="4-sdca-as-variance-reduction">[4] SDCA as Variance Reduction</h2>

<p>SDCA와 SAG는 분산 감소라는 측면에서 SVRG와 연결되어 있다. SDCA에서는 dual variable $\alpha_i^* =-\frac{1}{\lambda n}\triangledown\phi_i(w_* )$, $w^{(t)}=\sum_{i=1}^n\alpha_i^{(t)}$를 이용해서 analysis 한다. SDCA도 SVRG와 비슷하게 $\eta_t$가 $0$으로 가지 않아도 분산이 수렴한다.</p>

<h2 id="5-experiments">[5] Experiments</h2>

<p>Experiments에서는 SVRG를 SGD와 SDCA와 convex한 환경, nonconvex인 환경에서 비교하였다. SVRG의 weight는 SGD를 1번(convex), 10번(nonconvex) 돌려서 초기화 하였다. convex 문제에서 SGD보다는 확실히 좋았고, SDCA와는 비슷했지만 분산 감소 면에서는 SVRG가 더 좋았다. nonconvex에서는 SGD보다는 확실히 좋았고, SDCA와의 비교에서는 좋은 것도 있었고 나쁜 것도 있었다. 그런데 SVRG가 더 안정적으로 수렴하는 것처럼 보인다. SDCA와 SAG는 neural network에는 적용이 불가능하다.</p>

<hr />

<p>SAGA 논문에서는 이 논문이 SAG가 분산 감소를 해준다고 하지만, 자세한 설명은 없다고 나와있음.</p>

</article>



<section class="tags">
  <strong>Tags:</strong> <a href="/tag/optimization">Optimization</a>
</section>



<section class="rss">
  <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
</section>

<!-- <section class="share">
  <span>Share: </span>
  
    
    
    
    
    
    
    
    
  
</section> -->




<section class="disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_shortname = 'true';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



</div>
</div>

    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h3 class="footer-heading">YeonjeeJung's Blog</h3>

    <div class="site-navigation">

      <p><strong>Site Map</strong></p>
      <ul class="pages">

        <li class="nav-link"><a href="/about">About</a>
        <li class="nav-link"><a href="/category/thesis">Thesis</a>
        <li class="nav-link"><a href="/category/lecture">Lecture</a>

      </ul>
    </div>

    <div class="site-contact">

      <p><strong>Contact</strong></p>
      <ul class="social-media-list">
        <li>
          <a href="mailto:gotnwlsl@gmail.com">
            <i class="fa fa-envelope-o"></i>
            <span class="username">gotnwlsl@gmail.com</span>
          </a>
        </li>

        
          
          <li>
            <a href="https://github.com/YeonjeeJung" title="Fork me on GitHub">
              <i class="fa fa-github"></i>
              <span class="username">YeonjeeJung</span>
            </a>
          </li>
          
        

      </ul>
    </div>

    <div class="site-signature">
      <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
      <p class="text"></p>
    </div>

  </div>

</footer>

<!-- Scripts -->
<script src="//code.jquery.com/jquery-1.11.2.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/js/lightbox.min.js"></script>

<script type="text/javascript">
$(document).ready(function() {
  // Default syntax highlighting
  hljs.initHighlightingOnLoad();

  // Header
  var menuToggle = $('#js-mobile-menu').unbind();
  $('#js-navigation-menu').removeClass("show");
  menuToggle.on('click', function(e) {
    e.preventDefault();
    $('#js-navigation-menu').slideToggle(function(){
      if($('#js-navigation-menu').is(':hidden')) {
        $('#js-navigation-menu').removeAttr('style');
      }
    });
  });
});

</script>






  </body>

</html>
