<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Accurate, Large Minibatch SGD - Training ImageNet in 1 Hour</title>
  <meta name="description" content="[Abstract]">
  
  <meta name="author" content="YeonjeeJung">
  <meta name="copyright" content="&copy; YeonjeeJung 2019">
  

  <!-- External libraries -->
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/styles/monokai-sublime.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/css/lightbox.css">

  <!-- Favicon and other icons (made with http://www.favicon-generator.org/) -->
  <link rel="shortcut icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/icons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/icons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/icons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/icons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/icons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/icons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/icons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/icons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/icons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/icons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
  <link rel="manifest" href="/assets/icons/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/icons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">

  

  
  <!-- Twitter: card tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Accurate, Large Minibatch SGD - Training ImageNet in 1 Hour">
  <meta name="twitter:description" content="[Abstract]">
  <meta name="twitter:image" content="http://localhost:4000/assets/logo.png">
  <meta name="twitter:url" content="http://localhost:4000">
  

  

  <!-- Site styles -->
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/thesis/2019/11/26/IMG1HOUR.html">
  <link rel="alternate" type="application/rss+xml" title="YeonjeeJung's Blog" href="http://localhost:4000/feed.xml" />
</head>

  
    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$'] ],
    processEscapes: true,
  }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

  
  <body>

    <header class="navigation" role="banner">
  <div class="navigation-wrapper">
    <a href="/" class="logo">
      
      <img src="/assets/logo.png" alt="YeonjeeJung's Blog">
      
    </a>
    <a href="javascript:void(0)" class="navigation-menu-button" id="js-mobile-menu">
      <i class="fa fa-bars"></i>
    </a>
    <nav role="navigation">
      <ul id="js-navigation-menu" class="navigation-menu show">
        <li class="nav-link"><a href="/about">About</a>
        <li class="nav-link"><a href="/category/thesis">Thesis</a>
        <li class="nav-link"><a href="/category/lecture">Lecture</a>
          <ul>
            <a href="/category/computervision">CV</a>
            <a href="/category/optimization">Optimization</a>
          </ul>
        </li>
      </ul>
    </nav>
  </div>
</header>


    <div class="page-content">
        <div class="post">

<div class="post-header-container " >
  <div class="scrim ">
    <header class="post-header">
      <h1 class="title">Accurate, Large Minibatch SGD - Training ImageNet in 1 Hour</h1>
      <p class="info">by <strong>Yeonjee Jung</strong></p>
    </header>
  </div>
</div>

<div class="wrapper">

 <span class="page-divider">
  <span class="one"></span>
  <span class="two"></span>
</span>
 

<section class="post-meta">
  <div class="post-date">November 26, 2019</div>
  <div class="post-categories">
  in 
    
    <a href="/category/thesis">Thesis</a>
    
  
  </div>
</section>

<article class="post-content">
  <h2 id="abstract">[Abstract]</h2>

<p>딥러닝에서 네트워크와 데이터셋이 커지면 학습시간이 늘어나게 되어서 분산병렬 SGD를 사용한다. 이 논문에서는 미니배치 크기가 크면 최적화하는 데에 어려움을 야기하지만, 이렇게 훈련된 네트워크는 일반화를 잘 한다는 것을 보여준다. 또한 한번에 미니배치 사이즈를 8192개까지 늘려도 정확도에는 변함이 없다는 것을 보여준다. 이 결과를 얻기 위해 learning rate를 정하는 규칙을 적용했고, 새로운 warmup 방법을 만들었다.</p>

<h2 id="2-large-minibatch-sgd">[2] Large Minibatch SGD</h2>

<h3 id="21-learning-rates-for-large-minibatches">[2.1] Learning Rates for Large Minibatches</h3>

<p>이 논문의 목적은 정확도는 유지하면서 큰 미니배치 사이즈를 사용하는 것이다. 이들은 Linear Scaling Rule이라고 불리는, 미니배치 크기가 $k$배 커지면 learning rate도 $k$배 크게 해야 한다는 규칙을 발견했다. 작은 미니배치를 사용했을 때의 $w_{t+k}$와, $k$배의 미니배치를 사용했을 때의 $\hat{w}_{t+1}$이 비슷해지려면 $\hat{\eta}=k\eta$가 되어야 한다. 이렇게 학습시켰을 때, 둘의 정확도와 학습곡선이 비슷하다. 그러나 학습 초기에는 비슷하지 않을 수 있고, 미니배치 사이즈는 계속해서 커질 수 없다는 성질이 있다. 이 논문은 전례 없는 미니배치 사이즈를 가지고 실험적으로 Discussion에 있는 이론들을 테스트해본다.</p>

<h3 id="22-warmup">[2.2] Warmup</h3>

<p>constant warmup은 학습 초기 epoch에서 constant learning rate를 사용한다. 그러나 이 논문에서는 충분하지 않아서 gradual warmup을 사용했는데, 훈련 초반에 잘 수렴하도록 해준다.</p>

<h3 id="23-batch-normalization-with-large-minibatches">[2.3] Batch Normalization with Large Minibatches</h3>

<p>BN을 사용하면 원래 집합 $X$안에 있는 크기가 $n$인 모든 부분집합들을 모두 포함하고 있는 것처럼 생각할 수 있다. 분산 컴퓨팅 환경에서 BN을 사용한다면 per-worker sample size가 $n$이고 total minibatch size가 $kn$일 때, $X^n$에서 골라낸 독립된 $k$개의 $\cal{B}_ j$로 볼 수 있다. <em>이게 결국 각 minibatch의 분산을 비슷하게 만들어준다는 수업때 하려던 내용인듯</em></p>

<h2 id="3-subtleties-and-pitfalls-of-distributed-sgd">[3] Subtleties and Pitfalls of Distributed SGD</h2>

<h3 id="weight-decay">Weight decay</h3>

<p>L2 regularization때문에 weight decay를 사용하는데, 이 항이 원래의 loss에 더해진다. 따라서 원래 loss를 scaling 하는 것과 learning rate를 scaling하는 것은 다르다.</p>

<h3 id="momentum-correction">Momentum correction</h3>

<p>momentum을 사용한 SGD가 최근 많이 이용되고 있는데, learning rate가 바뀌면 momentum correction term을 적용해야 한다. <em>이게 왜 문제점인지 잘 모르겠음</em></p>

<h3 id="gradient-aggregation">Gradient aggregation</h3>

<p>전체 loss를 계산할 때 평균을 사용하는데, 각 worker의 loss를 $n$으로 나누면 다시 한번 전체를 $k$로 나누어야 하기 때문에, 그냥 처음부터 각 worker의 loss를 $kn$으로 나눠서 최종적으로는 다 더하기만 한다.</p>

<h3 id="data-shuffling">Data shuffling</h3>

<p>한 epoch당 데이터 전체를 $k$ worker에게 분배하는데, 모든 데이터들은 반드시 한번씩만 사용되도록 분배한다.</p>

<h2 id="4-communication">[4] Communication</h2>

<p>gradient aggregation은 backprop과 병렬로 처리되어야 한다. 한 layer의 gradient가 계산되면, 모든 worker에게서 정보를 받아서 합치고, 그와 동시에 다음 layer의 gradient가 계산될 수 있다.</p>

<h3 id="41-gradient-aggregation">[4.1] Gradient Aggregation</h3>

<p>gradient aggregation에는 allreduce 연산이 사용된다. 이 논문에서의 구현은 세 부분으로 되어 있다.</p>

<ol>
  <li>8개의 GPU에 있는 버퍼들을 각 서버에서 하나로 합친다.</li>
  <li>결과 버퍼가 공유되고 합쳐진다.</li>
  <li>결과가 각 GPU로 퍼진다.</li>
</ol>

<p>1번과 3번은 NCCL을 이용하였다. 2번에 대해서는 recursive halving and doubling 알고리즘과 bucket 알고리즘을 구현했는데, 큰 사이즈의 요소에 대해서는 recursive halving and doubling 알고리즘이 더 좋았다.</p>

<p>having/doubling 알고리즘은 reduce scatter collective와 allgather 단계로 구성된다. reduce scatter에서는 서버가 2개씩 짝을 지어, 1번 서버의 경우 자신의 버퍼의 뒤쪽 반을 보내고 2번 서버의 앞쪽 반을 받는다. 따라서 2의 승수개의 서버가 필요하다. 수신 및 전송하는 데이터가 reduction 되는 동안 목적지까지의 거리는 2배가 된다. 이 단계 이후에는 각 서버에는 최종 reduction된 데이터의 일부가 있다. allgather 단계에서는 통신을 역추적해서 각 서버에 있는 벡터들을 모은다. 각 서버에서 처음에 보내는 부분이 allgather에서 수신되고 있고, 그 이후에는 받았던 부분을 보낸다.</p>

<h3 id="42-software">[4.2] Software</h3>

<p><em>이부분이 사실 메인인 것 같은데.. 이해가 안감ㅠㅠ</em></p>

<h3 id="43-hardware">[4.3] Hardware</h3>

<p>이 논문에서는 50Gbit 네트워크가 ResNet-50을 학습시키는 데에 충분하다고 한다. 게다가 이 최대 bandwidth는 backprop에만 사용되고, forward pass에는 그만큼 쓰이지 않는다.</p>

<h2 id="5-main-results-and-analysis">[5] Main Results and Analysis</h2>

<p>이 논문의 메인 결과는 ImageNet에 ResNet-50과 256worker를 이용해 정확도 손실 없이 한시간만에 학습시켰다는 것이다. Linear scaling rule과 warmup 전략을 사용해서 이가 가능했다.</p>

<h3 id="52-optimization-or-generalization-issues">[5.2] Optimization or Generalization Issues?</h3>

<p>이 논문에서는 no warmup, constant warmup, gradual warmup을 실험했는데, training error와 validation error 면에서 모두 gradual warmup이 가장 성능이 좋았다. 또한 baseline의 learning curve도 초반 빼고는 잘 따라갔다.</p>

<h3 id="53-analysis-experiments">[5.3] Analysis Experiments</h3>

<p>minibatch size가 8k보다 작거나 같을 때에는 learning curve가 거의 비슷했는데, 그 이상에서는 비슷하지도 않았다. 여기서 알 수 있는 것은, learning curve를 비교해 보면 training이 끝나기 전에 결과가 잘 나올지, 아닐지 알 수 있다는 점이다.</p>

<p>minibatch size가 작았을 때는 $\eta=0.1$언저리에서는 비슷한 성능이 나왔지만, size가 커질수록 $\eta$를 더 섬세하게 바꿔야 한다.</p>

<p>BN에서의 일반적인 초기화는 $\gamma=1$인데, 이 논문에서는 각 residual block에 대해서만 $\gamma=0$으로 초기화했더니 더 좋은 결과가 나왔다.</p>

<p>ImageNet-5k dataset에 대해서도 실험해봤는데, 정확도가 작은 minibatch size와 정확도가 비슷한 minibatch size는 8k까지로, 데이터셋의 크기와 minibatch size의 관계는 없는 것으로 관찰되었다.</p>

<h3 id="54-generalization-to-detection-and-segmentation">[5.4] Generalization to Detection and Segmentation</h3>

<p>ImageNet을 학습시킨 ResNet-50을 R-CNN의 초기화에 이용해보았는데, $8k$까지는 성능이 나빠지지 않았다. linear scaling rule과 large batchsize는 다른 모델에서도 잘 적용된다는 것을 보여준다.</p>

<h3 id="55-run-time">[5.5] Run Time</h3>

<p>batch size를 늘리면 GPU간의 소통에도 불구하고 iteration 당 시간은 비슷하지만, epoch당 걸리는 시간이 선형적으로 줄어든다. 또한 GPU가 k배 늘어나면 초당 처리되는 이미지 수도 k배 늘어나는 것이 이상적인데, 이 논문의 방법은 GPU간의 소통에도 불구하고 이상의 $90%$정도를 달성했다.</p>

<hr />

<p>allreduce operation</p>

</article>



<section class="tags">
  <strong>Tags:</strong> <a href="/tag/optimization">Optimization</a>
</section>



<section class="rss">
  <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
</section>

<!-- <section class="share">
  <span>Share: </span>
  
    
    
    
    
    
    
    
    
  
</section> -->




<section class="disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_shortname = 'true';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



</div>
</div>

    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h3 class="footer-heading">YeonjeeJung's Blog</h3>

    <div class="site-navigation">

      <p><strong>Site Map</strong></p>
      <ul class="pages">

        <li class="nav-link"><a href="/about">About</a>
        <li class="nav-link"><a href="/category/note">Note</a>
        <li class="nav-link"><a href="/category/thesis">Thesis</a>
        <li class="nav-link"><a href="/category/lecture">Lecture</a>

      </ul>
    </div>

    <div class="site-contact">

      <p><strong>Contact</strong></p>
      <ul class="social-media-list">
        <li>
          <a href="mailto:gotnwlsl@gmail.com">
            <i class="fa fa-envelope-o"></i>
            <span class="username">gotnwlsl@gmail.com</span>
          </a>
        </li>

        
          
          <li>
            <a href="https://github.com/YeonjeeJung" title="Fork me on GitHub">
              <i class="fa fa-github"></i>
              <span class="username">YeonjeeJung</span>
            </a>
          </li>
          
        

      </ul>
    </div>

    <div class="site-signature">
      <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
      <p class="text"></p>
    </div>

  </div>

</footer>

<!-- Scripts -->
<script src="//code.jquery.com/jquery-1.11.2.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/js/lightbox.min.js"></script>

<script type="text/javascript">
$(document).ready(function() {
  // Default syntax highlighting
  hljs.initHighlightingOnLoad();

  // Header
  var menuToggle = $('#js-mobile-menu').unbind();
  $('#js-navigation-menu').removeClass("show");
  menuToggle.on('click', function(e) {
    e.preventDefault();
    $('#js-navigation-menu').slideToggle(function(){
      if($('#js-navigation-menu').is(':hidden')) {
        $('#js-navigation-menu').removeAttr('style');
      }
    });
  });
});

</script>






  </body>

</html>
