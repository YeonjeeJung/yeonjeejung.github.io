---
layout: post
title: Peeking Inside the Black-Box ; A Survey on Explainable Artificial Intelligence (XAI) - 2
author: Yeonjee Jung
tags : XAI
---

# AXIS 1. XAI Methods Taxonomy : Explainability Strategies

현재 있는 설명 방법들의 오버뷰를 제시할 것이다. 세 가지의 기준을 가지고 나눌 수 있는데,  
A. 해석의 복잡도  
B. 해석의 범위  
C. 사용된 ML 모델에 대한 의존성  
이다.

## A. Complexity Related Methods

모델의 복잡도는 직접적으로 해석력과 연관되어 있다. 일반적으로 더 복잡하면 더 설명하기 어렵다. 따라서 설명가능한 모델을 얻는 직접적인 방법은 본질적으로 설명가능한 알고리즘을 디자인하는 것이다.

BRL 모델은 결정 트리 기반의 모델인데, 저자는 초기의 설명가능한 모델이 XAI에 더 적합하다고 주장한다. 시각화를 통해 결과를 설명할 수 있는 attention model과, SLIM이라고 불리는 데이터 기반의 점수 시스템을 제안하는 연구도 있다. 최근에는 해석력과 정확도의 trade-off를 강조하며 '설명가능한 모델은 정확도 하락이라는 cost를 갖는다'를 주장하는 연구들도 활발하다.

설명가능한 모델을 만드는 대신, 복잡하고 높은 정확도를 갖는 블랙박스 모델을 만들고 역공학을 통해 설명을 제공하는 연구도 있다. 이 방법은 매우 복잡하고 비싸지만, 최근 XAI에서는 자연어, 시각화, 예시를 통한 설명이라는 분야들이 활발히 연구되고 있다. 만약 높은 정확도가 필수적이라면, 이 연구가 꼭 필요하다.

## B. Scoop Related Methods

모델의 해석력은 두가지의 범위가 있다.
1. 모델 전체의 행동을 이해 (Global interpretability)
2. 하나의 예측을 이해 (Local interpretability)

### 1. Global Interpretability
전체 해석력은 모델의 전체 논리를 이해하고 전체 모든 가능한 결과에 대해 추론하게 해준다. 많은 대상에 대한 추론에 사용한다.

GRIP이라고 불리는, local explanation을 이용해 모델의 global 설명 트리를 만들 수 있는 방법이 제안되었다. 이 방법으로 모델이 정당하게 작동하고 있는지, 아니면 오버핏되었는지를 알 수 있다. global 정보 추출을 위한 감독 방법도 제안되었다. 이 연구는 DNN이 해석 가능한 패턴 기반의 모델과 결합되어 해석될 수 있다는 아이디어를 돕는다. Activation maximization(선호되는 인풋을 종합하는 것) 기반의 접근법도 제안되었는데, 이미지 인식을 위해 global 해석가능한 모델을 만들어준다.

global 해석력은 파라미터가 너무 많은 경우에 얻기 어렵다. 사람은 전체를 이해하기 위해 부분에 집중하는데, 따라서 local 해석력이 더 적용하기 좋다.

### 2. Local Interpretability
특정한 결정에 대한 설명이나 하나의 예측을 하는 것은 해석력이 local에서 작용한다는 것을 말한다. 이러한 해석의 범위는 특정한 예를 들어서 모델이 왜 이런 선택을 했는지를 정의하는 데에 사용된다.

LIME은 블랙박스 모델을 관심있는 특정 예측 근처에서 근사할 수 있다. LOCO는 local variable의 중요도를 알려주는, local explanation을 할 수 있는 또다른 유명한 방법이다. local gradient를 이용해 local decision을 설명할 수 있는 방법도 있는데, 이 방법을 이미지 분류에도 쓸 수 있다. 이미지 분류 시스템에서 최종 클래스를 찾을 때 중요한 region을 찾는 것은 일반적인 방법론인데, 이 region은 sensitivity map, saliency map, pixel attribution map 등으로 불린다.

모델의 예측을 각각의 특징에 대한 개별적인 기여로의 분해에 기반하여 예시를 통해 모델을 설명하는 방법을 제시한 연구도 있다. 원래의 예측과, 특정 특징들을 무시하고 예측했을 때의 차이를 측정한다. local 방법들을 결합해주는 Shapely Explanation이라고 불리는 방법을 제안한 연구도 있다.

유망한 연구들은 local과 global explanation의 장점들을 결합하는 연구들이다. 4가지의 가능한 결합들은
1. 일반적인 global explanation
2. 모델의 각 부분들이 어떻게 전체 결과에 영향을 주는지 (global explanation을 부분 수준에 적용)
3. 왜 모델이 예시 그룹에 대해 이런 결정을 했는지 (그룹 수준에 local explanation 적용)
4. 일반적인 local explanation
이다.

local 설명은 보통 DNN에서 쓰이지만, 학자들은 자신의 방법이 DNN이 아니라 어떤 모델이든 적용될 수 있다고 말한다.

## C. Model Related Methods

위에서 설명한 것처럼 local 설명은 어떤 모델이든 적용될 수 있다고 했다. 이런 방법들은 Model-agnostic 설명력이라고 하고, 특정 모델에만 쓰일 수 있는 방법들은 Model-specific 설명력이라고 한다.

### 1. Model-specific Interpretability
이 설명력의 단점은, 우리가 특정한 타입의 설명이 필요할 때 그 설명이 가능한 모델만 사용할 수 있고, 더 정확도가 높거나 표현력이 좋은 모델을 사용할 수 없다는 비용이 든다는 점이다. 따라서 최근에는 model-agnostic 방법을 쓰는 것이 유행이다.

### 2. Model-Agnostic Interpretability
이 종류에 속하는 방법들은 설명과 예측을 분리시킨다. 이들은 주로 사후 분석(post-hoc)이다. 4가지의 종류로 나눠서 설명할 것이다.

a) Visualization  
b) Knowledge Extraction  
c) Influence Methods  
d) Example-based Explanation  

#### a) Visualization
DNN의 내부를 이해하기 위한 가장 일반적인 아이디어이다. 시각화는 반드시 감독학습에 적용되어야 한다. 대표적인 시각화 방법은  
(1) Surrogate models  
(2) Partial Dependence Plot(PDP)  
(3) Individual Conditional Expectation (ICE)  
이다.

(1) 근사 모델은 복잡한 모델을 설명하는 데에 사용되는 간단한 모델이다. 나중의 블랙박스 모델 해석을 위하여 예측 단계에 학습된다. 그러나 간단한 근사 모델이 복잡한 것보다 더 표현력이 뛰어나다는 연구 결과는 없다. LIME 방법은 하나의 관찰 근처에서 local 근사 모델을 만드는 방법이다. 모델 행동을 표현해주는 결정트리를 뽑아주는 근사 방법 연구가 있다. 또한 근사 모델을 가지고 계층 시각화를 하는 방법도 제안되었다.

(2) PDP는 하나 또는 그 이상의 변수와 블랙박스 모델의 예측 간의 평균적인 부분 관계를 시각화하는 데 도와주는 그래픽 표현이다. 감독학습 모델을 이해하기 위해서 PDP를 사용하는 연구들은 다음과 같다. Bayesian Additive Regression 예측기와 conditional average treatment effect 사이의 관계를 이해하기 위해 PDP를 사용한 연구가 있다. 생물학과 관련된 연구에서는, PDP를 stochastic gradient boosting에 사용했고, 비대칭 분류에서 랜덤포레스트와 PDP를 사용해 결정자와 반응 관계를 알아보는 것이 의미있음을 증명한 연구도 있다. 최근에는 랜덤포레스트를 시각화하기 위한 Forest Floor라는 방법이 제안되었는데, 이 방법은 PDP보다 특징 기여에 더 의존한다. 이 방법이 더 좋은 이유는, PDP의 평균에 의해서는 상호작용을 알 수 없다는 점이다.

(3) ICE는 PDP를 확장한 것이다. PD는 대략적인 모델의 작동만을 보여주는데, ICE는 PDP 결과물을 흩트려서 상호작용과 개별 분산을 보여준다. 최근 연구들은 PDP보다는 ICE를 사용한다. 부분 중요도(partial importance, PI)와 개별 조건 중요도(individual conditional importance, ICI) 둘 다를 시각 툴로 사용한 local 특징 중요도 기반 방법을 제안한 연구가 있다.

#### b) Knowledge Extraction

특히 ANN에 기반을 둔 모델이라면, ML 모델이 어떻게 동작하는지 아는 것은 어렵다. 모델은 학습하면서 내부 변수를 변경한다. 따라서 ANN에서 설명을 추출한다는 것은, 학습으로부터 얻은 정보와 내부 표현을 해석한다는 뜻이 된다. 두 가지의 주된 방법이 있는데,  
(1) Rule Extraction  
(2) Model Distillation  
이다.

(1) 복잡한 모델에서 인사이트를 얻을 수 있는 방법은 규칙 추출이다. 인풋과 아웃풋을 이용해서 어떻게 ANN이 결정을 내렸는지를 근사하는 규칙을 설명해주는 방법들의 연구가 많다. 이것은 전통적인 인공지능 전문가 시스템에서 사용하던 방법이다. 세 가지의 주된 방법들이 있다.  
(a) Pedagogical(가르치는) rule extraction  
(b) Decompositional rule extraction  
(c) Eclectic(절충) rule extraction  

(b)는 ANN으로부터 학습된 각 유닛 수준에서 규칙을 추출하며 투명하게 보는 반면 (a)는 ANN을 블랙박스로 취급한다. (a)는 OSRE(Orthogonal Search-based Rule Extraction)에서 적용되었다. (c)는 (a)와 (b)를 융합한 방법이다.

(2) 다른 방법은 모델 압축이다. 이 방법은 모델의 압축된 정보를 선생님 역할의 딥 네트워크에서 학생 역할의 얕은 네트워크로 전달하는 방법이다. 모델 압축은 계산량을 줄이기 위해 제안되었으나 이후에는 해석을 위해 사용되었다. Interpretable Mimic Learning은 깊은 망의 성능까지 따라하면서 robust한 결정을 내릴 수 있는 특징을 학습하는 방법이다. DarkSight는 dark knowledge의 개념에서 감명받아 만든 블랙박스 분류기의 예측을 해석하는 시각화 방법이다. 이는 지식 압축, 차원 축소, 시각화 방법들을 결합한 것이다.

#### c) Influence Methods

이 부류의 방법들은 인풋이나 내부 변수를 바꾼 뒤 모델 결과에 얼마나 영향을 미치는지를 알아내어 특징들의 중요도나 관련성을 예측한다. 이 방법들은 시각화될 수도 있다. 세 가지의 주된 방법이 있다.  
(1) Sensitivity Analysis (SA)  
(2) Layer-wise Relevance Propagation (LRP)  
(3) Feature Importance  
이다.

(1) 민감도는 모델이 인풋이나 내부 변수들의 변화로부터 그 결과가 얼마나 영향을 받는지를 의미한다. SA는 데이터가 바뀌어도 내부 변수나 결과가 얼마나 안정되게 나오는지 검증하기 위해 사용된다. 모델의 데이터가 바뀌어도 모델이 안정적이라는 것은 믿음을 주기에 좋다. SA는 결과값 그 자체 대신 분산이 모델을 설명해 준다. 따라서 SA는 관계를 설명해 주는 것이 아니라 모델의 안정성을 테스트 하기 위해 사용된다. 또한 중요하지 않은 인풋 특징이나 시작점을 찾아서 없애는 도구로써 사용되고, 이후에 더 강력한 설명 방법을 사용할 수 있다.

(2) LRP는 말단 노드에서 입력 노드쪽으로 역전파한다. 중요한 특징은 관계 보존이고, SA와는 반대로 어떤 특징이 결과에 가장 중요한지 알려준다.

(3) 변수 중요도는 각 인풋 특징들이 얼마나 결과에 영향을 미치는지를 정량화한다. 모델 예측 에러의 증가는 특징의 중요도를 측정하기 위해 특징이 치환된 후에 계산된다. 중요한 특징을 치환한 뒤에는 모델 에러가 증가하고, 중요하지 않은 특징이 치환된다면 모델의 에러는 변하지 않는다. MCR(Model Class Reliance)은 모델에 상관없이 특징의 중요도를 계산할 수 있는 방법이다. SFIMP라는 특징 중요도를 계산하는 local 방법도 제안되었고, LOCO도 지역 변수 중요도를 사용했다.

#### d) Example-based Explanation

데이터셋에서 특정 예시들을 선택하여 모델의 행동을 설명한다. 이들은 어떤 모델이든 더 해석가능하게 만들기 때문에 대부분 모델에 상관없는 방법이다. model-agnostic과 다른 점은 내부 변수를 변경하거나 모델을 바꾸는 일 없이 예시들만 선택해서 모델을 설명한다는 점이다. 두 가지의 방법이 있는데,  
(1) prototype and criticism  
(2) Counterfactuals explanations  
이다.

(1) 프로토타입은 데이터들을 대표하는 예시들을 고르는 것이다. 각각이 어디에 속하는지는 프로토타입과 얼마나 비슷한지에 따라 결정되는데, 이것은 과일반화로 연결되기도 한다. 이를 피하기 위해 비평이라고 불리는, 프로토타입으로 잘 대표되지 않는 데이터들을 따로 뽑는다. MMD-critic은 프로토타임과 비평을 자동으로 찾아주는 비지도학습 방법이다.

(2) 반사실 설명은 전체 논리를 설명할 필요 없이 반대 결론으로 이끌 수 있는 최소 조건만을 알려준다. 중요한 점은 설명해주는 것이 아니라 반대 예시를 찾는 것이다.

## Conclusion

* model-agnostic의 최대 장점은 설명과 표현 수준에서 볼 때, 모델의 융통성이 높다는 점이다. 여러 다른 모델을 비교할 수 있다. 그렇지만 이들은 여러 근사 방법을 사용하여 표현되는데, 이 때 정확도가 떨어질 수도 있다. model-specific방법들이 더 직접적으로 표현되기 때문에 더 정확한 설명을 해줄 수도 있다.

* ML모델은 완전히 투명하거나/사후에 해석될 수 있다. 또한 local에서 해석되거나/global하게 해석될 수 있다. local 방법은 데이터에 초점을 맞추고, global 방법은 모델에 초점을 맞춘다. 신뢰 관점에서는, local 방법이 더 충실하다고 할 수 있다.
