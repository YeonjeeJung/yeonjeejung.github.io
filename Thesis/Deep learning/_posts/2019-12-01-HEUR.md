---
layout: post
title: A Closer Look at Deep Learning Heuristics - Learning Rate Restarts, Warmup and Distillation
author: Yeonjee Jung
tags : Optimization
use_math : true
---

## [Abstract]

현재의 딥러닝 모델들은 heuristic들로부터 좋은 결과를 냈다. 현재 이 경험적인 방법들은 linear interpolation이나 차원 감소를 통한 시각화들로 분석되는데, 이들은 각자의 단점을 가지고 있다. 이 논문에서는 mode connectivity, Canonical correlation analysis(CCA)를 가지고 knowledge distillation과 cosine restart, warmup을 재분석한다. 분석으로 얻은 결과는 다음과 같다.

1. cosine annealing이 잘 작동되는 이유로 설명된 것들이 실험적으로 아님을 보였다.  
2. warmup은 깊은 층의 가중치 변화를 하지 않게 만들며, 그냥 그 가중치들을 고정시켜도 warmup과 같은 효과를 얻는다.  
3. knowledge distillation에서 teacher가 공유한 latent knowledge는 더 깊은 층으로 분배된다.

## [1] Introduction

knowledge distillation은 teacher model을 먼저 학습하고, 더 작은 데이터셋을 가지고 student model을 학습하는데, loss function을 사용하는 것이 아니고 teacher를 따라하는 방식으로 학습한다.

Mode connectivity(MC)를 이용하면 네트워크의 local minima들을 piecewise-linear curve로 연결할 수 있는데, 이는 서로 다른 방법으로 얻은 local minima와 특성들이 연결되어 있음을 알 수 있다.

CCA를 전처리 단계에서 사용하여 네트워크의 활성화를 분석할 수 있다는 연구도 있었다.

## [2] Empirical Tools

### [2.1] Mode Connectivity

local minima들을 연결하면, 그들을 연결한 사이에도 비교적 작은 loss의 점들이 있다.

#### [2.1.1] Resilience of Mode Connectivity

보통은 서로 다른 훈련 방법(batch size가 다르거나, optimizer가 다르거나, 초기화가 다르거나, 정규화 텀이 다르거나)을 사용하면 서로 다른 점으로 수렴한다고 알고 있다. 이 논문에서는 기준 mode $G$를포함해, 서로 다른 훈련 방법을 사용하여 ${A, B, C, D, E, F, G}$의 mode를 얻었다. 그리고 mode connectivity 알고리즘을 사용하여 $A$부터 $F$까지를 $G$와 짝지어 각각의 curve를 만들어 내었다. 그리고 그 curve의 점마다의 validation accuracy를 봤는데, 마지막 epoch에서는 다들 비슷한 정확도를 보였다. 따라서 결국 각 minima를 잇는 선들도 정확도가 비슷하다.

### [2.2] CCA for Measuring Representational Similarity

CCA는 다변량 통계에서의 두 RV 집합간의 관계를 연구하는 전통적인 툴이다. 이전 연구에서는 CCA를 SVD나 DFT와 함께 이용해서 두 layer를 비교하는 방법이 제안되었다. 각 layer의 activation에서 나오는 값을 행렬로 만들고, 그들과 SVD를 이용해 layer간의 correlation을 계산한다. convolution layer인 경우에는 DFT를 사용한다. 이 방법은 서로 다른 네트워크를 비교할 뿐만 아니라 한 네트워크가 훈련 중 어떻게 변화하는지를 보고싶을 때 사용하기도 한다.

## [3] Stochastic Gradient Descent with Restarts (SGDR)

cosine annealing이 잘 되는 이유는 잘 모르는데, multi-modal 함수에서는 restart가 다른 지역을 더 탐색할 수 있게 해준다는 것이 하나의 설명이다. 실험 결과로는, restart가 일어나기 직전 minima들을 line segment로 연결한 점들은 두 점보다 높은 loss를 보였지만, learning rate를 줄인 minima들은 line segment로 연결해도 loss가 증가하지 않았다. 결과를 보니, 사실 SGDR이 local minima를 탈출하게 해준다는 결과는 보이지 않았다. 그리고 training loss와 validation loss가 달라서, train loss 입장에서는 loss가 높아져도, validation loss의 입장에서는 낮아져서 일반화가 잘 될 때도 있다.

## [4] Warmup Learning rate Scheme

이론적으로, SGD의 learning dynamics는 batch size와 learning rate의 비율에 의존한다. warmup은 학습 불안정성을 야기하지 않고 큰 learning rate를 사용할 수 있는 방법으로 제안되었다. 이 논문에서 연구하고자 한 것은 "어떻게 learning rate warmup이 서로 다른 layer에 영향을 미칠까?"이다. 이것을 연구하기 위해 CCA를 사용하였고, 세 가지의 scheme을 사용한 네트워크들의 차이점과 유사점을 조사하였다. LB+warmup과 SB에서 유사점이 나타났는데, 이것은 큰 batch size와 큰 learning rate을 사용할 때, warmup이 불안정한 큰 변화를 없애준다는 것을 시사한다. 이것을 더 알아보기 위해 warmup을 사용하지 않고 대신 처음 20 epoch동안 fully connected layer들을 고정시켜 봤는데, 또한 유사도가 높았고 validation 정확도가 warmup을 사용한 것보다 더 높았다.

## [5] Knowledge Distillation

knowledge distillation 또한 CCA를 사용하여 연구하였다. 이전에는 dark knowledge의 전이가, 최근에는 중요도의 해석이 knowledge distillation을 잘 되게 한다고 설명되어 왔다. 이 논문에서는 knowledge transfer가 네트워크의 특정 부분에 국한되는지, student and teacher 모델의 layer사이의 유사도와 student가 이 질문에 대한 해답을 줄 수 있는지에 대해 조사하였다. student model을 학습하고, 같은 구조를 가진 모델을 hard label을 가지고 학습하였다. 그리고 그들의 layer간의 유사도를 teacher model과 비교하였다. 결과는, 깊은 층에 비해 얕은 층에 대해서는 base model과 teacher간의 유사도와 student와 teacher간의 유사도가 비슷했다. 따라서 깊은 층에 대해서만 knowledge transfer가 일어난다고 할 수 있다. _얕은 층에서도 knowledge transfer가 일어나지만, 그게 알아내기 쉬운 knowledge라서 transfer가 되지 않은 것처럼 보이는 것은 아닐까?_

## [6] Discussion and Conclusion

이 연구는 learning heuristic을 설명했고, 또한 우연히 새로운 heuristic을 발견하였다.

---

처음부터 끝까지 실험, 실험, 실험.... 논문은 이렇게 쓰는거구나
