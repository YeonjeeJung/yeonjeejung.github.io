---
layout: post
title: Deep Residual Learning for Image Recognition
author: Yeonjee Jung
tags : Optimization
use_math : true
---

## [Abstract]

residual learning 이라는 framework을 만들었는데, 이 방법은 layer input에 대한 reference가 된다. 이 방법을 이용해서 더 깊은 네트워크를 더 쉽고 정확하게 학습시킬 수 있다. 여기다 앙상블을 쓴 결과는 ImageNet에서 3.57%의 에러를 갖는다.

## [1] Introduction

네트워크의 깊이가 적당히 깊으면 학습이 잘되는데, 계속 깊게하면 성능이 저하된다. 이는 일반화에 관한 것이 아니고 train error 또한 크다. 네트워크를 더 깊게 만드기 위해 shortcut connection이라는 것을 만들었다. 만약 identity mapping이 최적이라면 이 네트워크는 0으로 학습된다.

## [3] Deep Residual Learning

### [3.1] Residual Learning

원래의 learning되는 함수가 $\mathcal{H}(x)$라고 하면, shortcut connection을 이용해 학습되는 함수는 $\mathcal{F}(x)+x$가 된다. 이렇게 학습되면 더이상 학습이 필요하지 않은 경우는 identity mapping이 되기 때문에 더 얕은 네트워크보다 성능이 저하되지 않는다. 이렇게 하면 새로 네트워크를 학습시키는 것보다 identity를 참조해서 복잡한 부분을 더 쉽게 찾을 수 있다.

### [3.2] Identity Mapping by Shortcuts

building block은 다음과 같은 모양이다.

$$y = \mathcal{F}(x, \{W_i\})+x$$

plain network와 residual network는 같은 수의 파라미터, 깊이, 넓이, 계산복잡도를 갖는다. 만약 input과 output이 같은 차원을 갖지 않으면 그를 맞추기 위해 다음과 같이 설정한다.

$$y = \mathcal{F}(x, \{W_i\})+W_sx$$

그러나 identity mapping이 성능 저하를 줄이는 데에 좋으므로, 이런 형태는 차원 맞추기에만 사용된다. $\mathcal{F}$는 주로 그 안에 여러 층을 갖는다. 만약 한 층만 갖게 되면 선형 모양이 되기 때문에 plain network와 비교했을 때 별다른 이득을 얻지 못한다.

### [3.3] Network Architectures

이 논문에서의 baseline은 VGGnet에서 영감을 받았다.

## [4] Experiments

### [4.1] ImageNet Classification

34layer net 안에 18layer net의 모든 경우의 수가 들어갈 수 있음에도 불구하고 34layer net의 train error가 더 작았다. 따라서 degradation 현상을 목격했다고 할 수 있다. 이 논문에서는 실험에 BN을 사용했기 때문에 gradient vanishing 현상에 의해 나타났다고 보기 어렵다. *이 논문에서는 deep plain net이 수렴도가 매우 낮기 때문이라고 생각한다.*

ResNet에서는 더 깊은 네트워크가 train error가 더 작았고, 일반화도 더 잘 되었다. 또한 ResNet이 더 수렴속도가 빨랐다. 세 가지 구조가 있는데, (A) 차원을 늘리기 위해 zero-padding shortcut을 사용하고, 모든 shortcut은 parameter-free (B) 차원을 늘리기 위해 projection shortcut을 사용하고 다른 shortcut은 identity (C) 모든 shortcut은 projection. 그러나 이 결과들이 아주 조금의 차이만 있기 때문에, 이 논문에서는 projection shortcut이 residual learning에서 필수적인 요소는 아니라고 결론짓는다.

이 논문에서는 3층의 bottleneck 구조를 사용하는데, 이 구조는 2층의 구조와 시간복잡도는 비슷하고, convolution에서의 input과 output의 크기는 줄여준다. 만약 여기서 parameter-free가 아닌 projection으로 대체된다면, 시간복잡도가 배가 될 것이므로 identity shortcut이 더 효율적이다. 이 구조를 이용해 더 깊은 50layer, 101layer, 152layer를 만들어냈는데, 이들은 degradation 문제가 없으며, 더 깊을수록 결과가 더 좋았다.

### [4.2] CIFAR-10 and Analysis

CIFAR-10에서는 warmup을 위해 초기 learning rate을 0.01로 두고 training error가 80%이하가 된 이후에 0.1로 높였고, 그 이후의 learning rate schedule은 이전과 같이 정했다. 이 논문에서는 layer마다 표준편차를 계산했는데, ResNet은 plain net보다 더 표준편차가 적은 것을 볼 수 있었다. 그리고 더 깊은 network일수록 표준편차가 더 작았다.

그러나 1000layer가 넘는 network에서는 그보다 적은 layer의 network와 같은 train error를 가져도 test error가 더 높았다. *이 논문에서는 이가 overfitting 때문이라고 생각한다. 그러나 이 논문에서는 dropout같은 정규화 방법을 쓰지 않았기 때문에 이 방법들을 사용하면 결과가 더 나아질 것이라고 본다.*

---

10-crop test : 이미지의 각 코너와 center crop을 하고, flip된 이미지도 마찬가지로 처리하면 10개이다.
