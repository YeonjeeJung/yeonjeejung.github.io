---
layout: post
title: Visualizing the Loss Landscape of Neural Nets
author: Yeonjee Jung
tags : Optimization
use_math : true
---

## [Abstract]

이 논문에서는 왜 특정 네트워크 구조가 학습이 잘되고 일반화가 잘되는지를 알기 위해, loss function의 구조와 지형을 시각화해보고자 한다.

## [1] Introduction

이 논문에서 알아보고자 하는 것은  
1. loss function의 특징  
2. 네트워크 구조가 loss지형에 미치는 영향  
3. non-convex 구조가 학습에 미치는 영향  
4. loss function의 지형이 일반화 특성에 미치는 영향

이다. 그리고 이들은 filter normalization이라는 방법을 사용했다.

## [2] Theoretical Background

연구자들은 sharpness나 flatness가 일반화 능력에 관련있다고 했다. 그래서 flatness를 최소값 근처의 면적, hessian의 eigenvalue 또는 sharpness를 local entropy를 이용해서 나타내기도 했으나, [다른 연구](https://arxiv.org/abs/1706.08947)에서는 일반화 능력은 이런 flatness의 측정값과는 관련이 없다고도 했다.

## [3] The Basics of Loss Function Visualization

loss function을 시각화하는 데에는 1차원 방법과 2차원 방법이 있다. 1차원 방법은 두 점을 찍어서 그 두 점을 잇는 점들의 loss 값을 그래프로 그리는 방법인데, non-convexity를 시각화하기 어렵고, batch normalization이나 invariance symmetry(또는 scale invariance)를 고려하지 않는다. 2차원 방법은 한 점과 특정 두 방향을 정해서 그 길을 따라가며 2차원으로 그래프를 그리는 방법인데, 계산량이 많아서 resolution이 낮아 역시 non-convexity를 시각화하기 어렵다.

## [4] Proposed Visualization : Filter-Wise Normalization

network weight에는 scale invariance라는 속성이 있는데, 이는 weight에 특정 수를 곱하거나 나누어도 전체 네트워크의 행동이 변하지 않는다는 것이다. 이런 특성 때문에 weight값이 한 unit이 바뀌어도, 원래의 scale이 컸으면 영향이 거의 없고, 원래의 scale이 작았으면 엄청난 영향이 된다. 따라서 여러 다른 minimizer나 다른 네트워크를 비교할 수 없게 된다. 이런 문제를 해결하기 위해 이 논문에서는 filter-wise normalization을 제안하는데, 위의 2차원 방법을 사용하되 두 방향을 고른 뒤 $$d_{i,j}\leftarrow \frac{d_{i,j}}{\|d_{i,j}\|}\|\theta_{i,j}\|$$로 normalize하는 방법이다. 이렇게 하면 본래의 거리 scale을 유지할 수 있다.

## [5] The Sharp vs. Flat Dilemma

1차원 방법으로 small batch의 minimizer $\theta^s$와 large batch의 minimizer $\theta^l$을 그려본 결과, $\theta^s$가 더 flat했고, 실제로 small batch를 사용하면 일반화가 더 잘된다. 하지만 weight decay를 적용하였더니 $\theta^l$이 더 flat했다. 사실 $\theta^s$의 weight들이 크기가 더 크다. 따라서 scale variance때문에 더 flat해보이기만 한 것이다. 마찬가지로 weight decay에서는 shrinkage 효과때문에 $\theta^s$의 weight 크기가 작아졌고, 따라서 $\theta^l$이 더 flat해보인 것이다. 따라서 sharpness와 일반화는 관련이 없다고 결론지을 수 있다.

이 실험을 filter-wise normalization을 이용해서 다시 해봤는데, 비교해본 결과 weight decay를 사용한 것과 사용하지 않은 것에서 둘다 $\theta^s$이 더 flat했다. 따라서, 사실은 일반화는 sharpness와 관련이 있었지만, 여태까지의 시각화가 잘못된 것임을 알 수 있다.

## [6] What Makes Neural Networks Trainable? Insights on the (Non)Convexity Structure of Loss Surfaces

ResNet과, ResNet과 같은 구조인데 skip connection이 없는 네트워크를 filter-wise normalization을 이용해 비교해본 결과 관찰된 특성이 몇가지 있다.

### 1. 네트워크 깊이의 효과

ResNet-20 with no skip connection은 convex한 모습을 띠는데, 이는 VGG-19와 비슷한 구조로, 원래 학습이 잘되는 구조이다. 그런데 ResNet-56와 ResNet-110 with no skip connection은 복잡하고 non-convex한 모양을 띤다. 반대로 그냥 ResNet-56과 ResNet-110은 여전히 convex한 모양을 띤다. 이것으로, 얕은 네트워크에서는 skip connection의 효용이 두드러지지 않지만, 깊은 네트워크에서는 매우 중요한 역할을 한다고 볼 수 있다.

### 2. Wide model vs. Thin model

WRN의 k를 늘려가며 실험했는데, 더 wide한 네트워크가 더 flat했다. skip connection이 없는 모델에서도 더 넓은 모델이 더 flat한 지형을 갖는다. 또한, 더 wide한 모델의 테스트 정확도가 높았으므로, flatness와 일반화의 관계를 다시한번 확인할 수 있다.

### 3. 네트워크 초기화

convex에 가까운 ResNet 네트워크나 VGG 네트워크의 경우, 랜덤하게 초기화해도 잘 최적화될 가능성이 높지만, 깊은 ResNet with no skip connection 네트워크의 경우 랜덤 초기화는 gradient가 전혀 정보를 주지 않는 곳에 있을 확률이 높아 학습하기 어렵다.

### 4. Convexity

우리가 본 컨투어는 차원을 엄청나게 낮춘 것이기 때문에, non-convex처럼 보이면 실제 차원에서도 non-convex하지만, convex처럼 보인다고 해서 실제 차원에서 convex인 것은 아니고 양의 curvature들이 우세하다는 정도로만 보아야 한다. 이것을 보이기 위해 이전에 그린 그래프를 따라 $$\|\frac{\lambda_{min}}{\lambda_{max}}\|$$를 그려보았는데, convex같아보이는 그래프에서는 이 값들이 작았고, 아닌 그래프에서는 non-convex로 보이는 부분들에 이 값이 크게 나타났다. 
