---
layout: post
title: Automatically Inferring Data Quality for spatiotemporal Forecasting
author: Yeonjee Jung
tags : Optimization
use_math : true
---

## [Abstract]

시공간 데이터가 많이 쓰이고 있는데, 이때 데이터의 질이 다양할 수 있다. 이는 잘못하면 신뢰도가 떨어지는 예측으로 이어질 수 있고, 이는 블랙박스인 딥러닝에서는 치명적일 수 있다. 따라서 이 논문에서는 데이터의 질을 자동으로 알려주는 해결책을 제시한다.

## [1] Introduction

여러 다른 센서의 사용으로 데이터에 다양한 노이즈가 낄 수 있고, 따라서 네트워크의 성능 저하가 생길 수 있다. 이 논문에서는 데이터의 질을 예측하는 DQ-LSTM 모델을 제시하는데, 이 모델은 data quality level에 대한 공간적 구조를 잘 탐색할 수 있고, 각 시계열에 대한 의존도도 파악할 수 있다. 관련 연구는 서로 다른 출처에서의 데이터의 질에 대한 연구와, 그래프의 신호를 학습하는 연구가 있었다. 데이터의 질에 관한 연구들은 대부분 label이 필요했는데, 이 논문의 방법에서는 label을 사용하지 않고 서로 다른 출처의 데이터를 구분할 수 있다. 그래프의 신호를 학습하는 연구에서는 특별히 CNN에 관한 연구가 있었는데 이는 지역화된 패턴들을 뽑을 수 있는 구조이다. 이 논문에서는 graph convolution layer를 사용하여 시공간 특징을 data quality에 맵핑하는 모델을 제안한다.

## [2] Preliminaries

### [2.1] Local Variation

local variation은 원래 $\triangledown_ix=(\frac{\partial x}{\partial e}\|_ {e=(i,j)}\|j\in \mathcal{N}_i)$라고 정의한다. 이때 $\mathcal{N}_i$는 $i$번째 vertex에 연결된 vertex들인데, vertex마다 연결된 개수가 다르면 차원이 일정하지 않을 수 있다. 따라서 최종적으로

$$\|\triangledown_ix \|_ 2^2=\sum_{j\in\mathcal{N}_i}W_{ij}(x(j)-x(i))^2$$

라고 정의된다. 이 정의에 따르면, 연결된 vertex의 $x$값 (시그널)들이 현재 vertex의 $x$값과 비슷하면 local variation은 작아진다. 또한, local variation은 $W$와 $x$의 함수라는 것도 알 수 있다.

또한 $M$개의 서로 다른 센서들에서 온 신호가 있다고 할 때, 한 vertex의 local variation을

$$L_i=(\|\triangledown_ix^1\|_ 2^2,\cdots,\|\triangledown_ix^m\|_ 2^2, \cdots, \|\triangledown_ix^M\|_ 2^2)$$

로 나타낼 수 있다. 최종적으로,

$$L(W, X)=(D+W)(X\odot X)-2(X\odot WX)$$

라고 할 수 있다. ($\odot$은 element-wise multiplication)

### [2.2] Data Quality Level

data quality level을 정할 때는 주위 신호와 현재 node의 신호가 큰 차이가 없는 것이 바람직하다고 가정한다. data quality level은 $s_i=q(L_i)$라고 할 수 있는데, 함수 $q$는 임의로 정할 수 있다. GCN이라는 그래프 파라미터를 학습할 수 있는 CNN 모델이 제안되었는데, 이 모델을 여러겹 쌓은 모델을 만들어 통과시킨 후 data quality level을 결정할 수 있다.

$$Z=\sigma_K(\hat{A}\sigma_{K-1}(\hat{A}\cdots\sigma_1(\hat{A}X\Theta_1)\cdots\Theta_{K-1})\Theta_K)$$  
$$s=\sigma_L(L(W,Z)\Phi)$$

## [3] Model

### Data quality network

다층의 GCN이 그래프 신호와 DQL 사이에서 연관 신호를 추출하고 필터 크기를 늘릴 수 있다. 이 DQN으로 단일층 뉴럴 네트워크 $\Phi$와 활성함수 $\sigma_L$을 사용한다.

### Long short term memory

순간적인 신호를 처리하기 위해 이 논문에서는 LSTM을 사용했다. 길이가 $k$인 순차 신호를 LSTM에 넣고 다음 신호를 예측한다. 예측된 신호는 실제 신호와 비교되며 LSTM의 파라미터가 업데이트된다.

### DQ-LSTM

LSTM에 신호를 넣기 위해서는 전체 신호를 길이가 $k$인 신호로 세그먼트를 만들어야 한다. 그리고 모든 vertex에 대한 마지막 신호는 GCN의 입력이 된다. loss function은 다음과 같다.

$$\mathcal{L}_i=\frac{1}{n_i}\sum_{j=1}^{n_i}s_i\|\hat{\mathcal{X}}(i,:,k+j-1)-\mathcal{X}(i,:,k+j-1)\|_2^2+\beta\|\Phi\|_2^2$$

$i$는 vertex의 인덱스이고, $j$는 세그먼트의 시작점, $k$는 세그먼트의 길이, $\hat{\mathcal{X}}$는 예측신호값, $\mathcal{X}$는 실제 신호, $s_i$는 노드의 quality level이다.

## [4] Experiments

이 논문에서는 평가 척도로 mean absolute error (MAE) $\frac{\sum_{i=1}^n\|y_i-x_i\|}{n}$을 사용했다.

### [4.2] Graph Generation

weather station에 대해서 그래프를 만들어야 하는데, 문제는 어디를 연결해야 하는지 정하는 것이다. 처음에는 거리 기반으로 연결을 정의하려 했으나, 데이터에 거리 외에도 많은 특징들이 있기 때문에 거리가 가까워도 다른 특징들이 많이 차이가 나는 현상이 생길 수 있다. 따라서 이 논문에서는 모든 특징에 똑같은 중요도를 두고 연결을 정의했다.

### [4.3] Baselines

이 논문에서는 우선 확률론적 방법인 autoregressive와 비교하고, simple LSTM과 비교하고, 마지막으로 GCN과 비교한다.

## [5] Results and Discussion

### [5.1] Forecasting Experiments

이 논문에서의 결과에 따르면, $K$가 더 크면 더 큰 주변의 신호를 알게 되므로 결과가 더 좋아진다. GCN과 DQ-LSTM의 차이점은, GCN은 주어진 신호에서 data quality를 바로 예측하지만 DQ-LSTM은 local variation을 계산한 후 data quality를 예측한다. DQ-LSTM의 성능이 더 좋은 것으로 볼 때, local variation이 data quality 예측에 유용하게 쓰인다는 것을 알 수 있다.

### [5.2] Node Embedding and Low-Quality Detection

DQ-LSTM이 GCN과 연결되어 있으므로 GCN에서 나온 output을 가지고 embedding으로 표현할 수 있다. embedding은 낮은 차원을 갖고 있기 때문에 t-SNE와 같은 방법으로 시각화할 수 있다. 이 논문에서는 이 방법을 사용해 시각화했는데, 맨 처음에는 기준 노드에 연결된 노드들이 근처에 있다가, 한 노드가 자신과 연결된 다른 노드에 의해 영향을 더 받게 되면 기준 노드로부터 멀어진다.

위에서 기준 노드로부터 멀어진 노드를 $F$라고 하고, 그와 연결된 다른 노드를 $C$라고 하자. $F$가 기준 노드와 멀어진 이유는 두 가지로 생각해볼 수 있는데, 하나는 $F$의 신호가 기준 노드에 연결된 다른 노드들의 신호와 매우 다른 것이고, 다른 하나는 $C$의 신호에 노이즈가 많아서 $F$에 영향을 준 것이다. 첫번째 이유는 이전의 가정에 위배되므로 사실이 아니고, 따라서 $C$와 $F$가 low-quality node의 후보가 될 수 있다.
