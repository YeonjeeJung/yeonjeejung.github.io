<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>SGDR - Stochastic Gradient Descent with warm Restarts</title>
  <meta name="description" content="2016년">
  
  <meta name="author" content="YeonjeeJung">
  <meta name="copyright" content="&copy; YeonjeeJung 2023">
  

  <!-- External libraries -->
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/styles/monokai-sublime.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/css/lightbox.css">

  <!-- Favicon and other icons (made with http://www.favicon-generator.org/) -->
  <link rel="shortcut icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/icons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/icons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/icons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/icons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/icons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/icons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/icons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/icons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/icons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/icons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
  <link rel="manifest" href="/assets/icons/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/icons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">

  

  
  <!-- Twitter: card tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="SGDR - Stochastic Gradient Descent with warm Restarts">
  <meta name="twitter:description" content="2016년">
  <meta name="twitter:image" content="https://yeonjeejung.github.io/assets/logo.png">
  <meta name="twitter:url" content="https://yeonjeejung.github.io">
  

  

  <!-- Site styles -->
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://yeonjeejung.github.io/thesis/2019/10/27/SGDR.html">
  <link rel="alternate" type="application/rss+xml" title="YeonjeeJung's Blog" href="https://yeonjeejung.github.io/feed.xml" />
</head>

  
    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$'] ],
    processEscapes: true,
  }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

  
  <body>

    <header class="navigation" role="banner">
  <div class="navigation-wrapper">
    <a href="/" class="logo">
      
      <img src="/assets/logo.png" alt="YeonjeeJung's Blog">
      
    </a>
    <a href="javascript:void(0)" class="navigation-menu-button" id="js-mobile-menu">
      <i class="fa fa-bars"></i>
    </a>
    <nav role="navigation">
      <ul id="js-navigation-menu" class="navigation-menu show">
        <li class="nav-link"><a href="/about">About</a>
        <li class="nav-link"><a href="/category/thesis">Thesis</a>
        <li class="nav-link"><a href="/category/lecture">Lecture</a>
          <ul>
            <a href="/category/computervision">CV</a>
            <a href="/category/optimization">Optimization</a>
          </ul>
        </li>
      </ul>
    </nav>
  </div>
</header>


    <div class="page-content">
        <div class="post">

<div class="post-header-container " >
  <div class="scrim ">
    <header class="post-header">
      <h1 class="title">SGDR - Stochastic Gradient Descent with warm Restarts</h1>
      <p class="info">by <strong>Yeonjee Jung</strong></p>
    </header>
  </div>
</div>

<div class="wrapper">

 <span class="page-divider">
  <span class="one"></span>
  <span class="two"></span>
</span>
 

<section class="post-meta">
  <div class="post-date">October 27, 2019</div>
  <div class="post-categories">
  in 
    
    <a href="/category/thesis">Thesis</a>
    
  
  </div>
</section>

<article class="post-content">
  <p>2016년</p>

<hr />

<h2 id="abstract">[Abstract]</h2>
<p>재시작 방법은 gradient-free 최적화에서 멀티모달 함수에 적용할 때 자주 쓰인다. 부분적 재시작 또한 gradient기반 최적화에서 ill-conditioned 함수에서 수렴도를 개선하기 위해 자주 쓰이는 추세이다. 이 논문에서는 SGD를 위한 간단한 재시작 테크닉을 소개하는데, 딥네트워크를 학습시킬 때 항상(anytime) 결과를 향상시킬 수 있다.</p>

<h2 id="1-introduction">[1] Introduction</h2>
<p>GD를 쓸때 hessian을 쓰면 더 좋은데 계산량이 많다. AdaDelta와 Adam은 hessian을 잘 줄여서 사용한 좋은 예이다. 그런데 sota 결과는 사실 특별한 방법을 쓴게 아니라 SGD에 momentum만 추가한 것이었다.</p>

<p>보통의 learning rate schedule은 정해진 상수를 일정 간격의 상수로 나누는 것이었는데, 이 논문에서 제안하는 새로운 learning rate schedule은 주기적으로 SGD를 재시작하는 방법이다. 실험 결과에 의하면 재시작 방법은 원래 쓰이던 방법보다 2배에서 4배정도 적은 epoch만으로 비슷한 결과를 낼 수 있다.</p>

<h2 id="2-related-work">[2] Related Work</h2>
<p>gradient-free optimization에서는 많은 local optima를 찾는 것이 목적이다. niching 방법 기반 방법들은 local optimizer를 전체 space에 다 적용시킬 수 있는데, 차원의 저주 때문에 확장시킬 수는 없다. 최근에는 다양한 재시작 매커니즘들을 사용하는데, 한 방법에서는 많은 후보를 쓰면 더 글로벌한 검색이 가능한데, 각 재시작 처음엔 적은 후보를 쓰고 각 재시작 후에는 키우는 방법을 사용하는 것이 일반적이다.</p>

<p>gradient-based optimization에서는 gradient-free 에서보다 $n$배의 속도 향상이 있다. 이때 재시작 방법은 multimodality를 해결하기 위함보다는 수렴도를 개선하기 위해 사용된다.</p>

<h2 id="3-sgdr">[3] SGDR</h2>
<p>현존하는 재시작 방법은 SGD에도 적용될 수 있다. 데이터의 덩어리에 따라 loss value와 기울기가 다양할 수 있어서, 기울기나 loss의 평균을 내는 등의 노이즈의 제거가 필요하다.</p>

<p>이 논문에서는 지정된 epoch까지 도달하면 다시 재시작을 하는 가장 간단한 재시작 방법을 사용한다. 그리고 제안된 cosine annealing이라고도 불리는 learning rate schedule은 다음과 같다.</p>

<script type="math/tex; mode=display">\eta_t=\eta_{min}^i+\frac{1}{2}(\eta_{max}^i−\eta_{min}^i)(1 + \cos(\frac{T_{cur}}{T_i}\pi))</script>

<p>여기서 $\eta_t$는 learning rate, $\eta_{min}^i$와 $\eta_{max}^i$는 learning rate의 범위, $T_{cur}$는 현재 epoch, $T_i$는 지정된 epoch(이만큼 지나면 재시작)이다.</p>

<p>재시작은 learning rate ($\eta_t$)을 증가시키므로써 수행되고, $x_t$는 초기 해로 사용된다. learning rate는 $\eta_{max}^i$부터 $\eta_{min}^i$까지 줄어들고, 정해진 epoch를 돌면 다시 처음부터 시작한다. $T_{mult}$라는 변수를 이용하여 재시작마다 줄어드는 간격을 점점 넓힐 수도 있다.</p>

<p>처음 재시작 전에는 $x_t$를 초기 해로 사용하지만, 그 다음에는 이전의 최소 learning rate로부터 얻어진 $x$를 초기 해로 사용한다. (이 점이 중요한 부분임)
<em>그런데 계속 저렇게 사용하는거? 아님 재시작마다 저렇게 사용하는거?</em></p>

<h2 id="4-experimental-results">[4] Experimental Results</h2>
<h3 id="42-single-model-result">[4.2] Single-Model Result</h3>
<p>$T_0=200$의 결과가 가장 좋은데, 가장 마지막 몇 epoch에서 좋아진다. $T_{mult}=2$는 재시작 후 주기를 2배로 늘려주는데, 이렇게 하는 이유는 좋은 테스트 에러에 가장 빨리 도달하기 위함이다. SGDR이 좋은 성능에 빠르게 도달할 수 있기 때문에, 더 큰 신경망을 학습시킬 수 있다. 따라서 WRN을 2배 넓게 만들어서 학습시켰다.</p>

<p>SGDR 자체 실험에서는 SGDR과 기본 스케줄을 비교했는데, 120 epoch까지는 더 빨리 training loss가 줄었다. 이 이후에 기본 스케줄은 overfit되었다. 결론적으로, SGDR은 overfit이 잘 되지 않는다.</p>

<h3 id="43-ensemble-results">[4.3] Ensemble Results</h3>
<p>SGDR은 WRN논문의 follow-up study에서 영감을 얻었다.  여기서는 재시작 전마다 snapshot을 찍고 그것으로 앙상블 모델을 만든다. 결과로는, 3번 돌려서 앙상블하는 것보다 한번 돌려서 3번 스냅샷 찍어서 앙상블하는게 낫다. SGDR에서 찍은 스냅샷은 앙상블을 할 때의 유용한 다양성을 제공해 준다. 이 결과는 WRN보다 더 좋은 모델에서 더 좋은 결과를 낼 것이다.</p>

<h3 id="45-preliminary-experiments-on-a-downsampled-imagenet-dataset">[4.5] Preliminary Experiments on a Downsampled ImageNet Dataset</h3>
<p>다운샘플된 이미지넷 데이터는 원래보다 더 어렵고 이미지의 대부분을 대상이 차지하는 CIFAR-10보다도 더 어렵다.</p>

<h2 id="5-discussion">[5] Discussion</h2>
<p>이 learnin rate schedule은 재시작 없이도 충분히 경쟁적이고, 단 두 개의 파라미터(초기 lr, epoch 수)만을 필요로 한다. 재시작 방법의 목적은 ‘항상(anytime)’ 좋은 성능을 내기 위함이다. 매 재시작마다 $\eta_{max}$와 $\eta_{min}$을 줄이는 방법도 가능하다. SGDR 중 얻어진 중간 모델은 앙상블에 사용될 수 있고, cost도 들지 않는다는 점을 이용했다.</p>

<h2 id="6-conclusion">[6] Conclusion</h2>
<p>WRN에서는 더 넓은 모델을 사용하고 스냅샷을 앙상블에 사용해 sota 결과를 만들어냈고, EEG에서는 더 재시작을 많이 하고 더 스냅샷을 많이 찍으면 더 좋은 결과를 낸다는 것을 알았다. Downsampled ImageNet 데이터에서는 SGDR이 scan을 통해 lr을 선택하는 문제를 줄여준다는 것을 알았다. 다음 연구는 AdaDelta나 Adam에 적용하는 것이 될 것이다.</p>

<hr />

<p>다음에 읽어볼 논문 : <a href="https://arxiv.org/abs/1704.00109">Snapshot</a>, <a href="https://arxiv.org/abs/1605.07146">WRN</a></p>

</article>



<section class="tags">
  <strong>Tags:</strong> <a href="/tag/optimization">Optimization</a>
</section>



<section class="rss">
  <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
</section>

<!-- <section class="share">
  <span>Share: </span>
  
    
    
    
    
    
    
    
    
  
</section> -->




<section class="disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_shortname = 'true';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



</div>
</div>

    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h3 class="footer-heading">YeonjeeJung's Blog</h3>

    <div class="site-navigation">

      <p><strong>Site Map</strong></p>
      <ul class="pages">

        <li class="nav-link"><a href="/about">About</a>
        <li class="nav-link"><a href="/category/thesis">Thesis</a>
        <li class="nav-link"><a href="/category/lecture">Lecture</a>

      </ul>
    </div>

    <div class="site-contact">

      <p><strong>Contact</strong></p>
      <ul class="social-media-list">
        <li>
          <a href="mailto:gotnwlsl@gmail.com">
            <i class="fa fa-envelope-o"></i>
            <span class="username">gotnwlsl@gmail.com</span>
          </a>
        </li>

        
          
          <li>
            <a href="https://github.com/YeonjeeJung" title="Fork me on GitHub">
              <i class="fa fa-github"></i>
              <span class="username">YeonjeeJung</span>
            </a>
          </li>
          
        

      </ul>
    </div>

    <div class="site-signature">
      <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
      <p class="text"></p>
    </div>

  </div>

</footer>

<!-- Scripts -->
<script src="//code.jquery.com/jquery-1.11.2.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/js/lightbox.min.js"></script>

<script type="text/javascript">
$(document).ready(function() {
  // Default syntax highlighting
  hljs.initHighlightingOnLoad();

  // Header
  var menuToggle = $('#js-mobile-menu').unbind();
  $('#js-navigation-menu').removeClass("show");
  menuToggle.on('click', function(e) {
    e.preventDefault();
    $('#js-navigation-menu').slideToggle(function(){
      if($('#js-navigation-menu').is(':hidden')) {
        $('#js-navigation-menu').removeAttr('style');
      }
    });
  });
});

</script>






  </body>

</html>
