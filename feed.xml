<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YeonjeeJung's Blog</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 03 Jan 2022 01:07:15 +0900</pubDate>
    <lastBuildDate>Mon, 03 Jan 2022 01:07:15 +0900</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>Optlecture12</title>
        <description>
</description>
        <pubDate>Thu, 08 Oct 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2020/10/08/OptLecture12.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2020/10/08/OptLecture12.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optimization Lecture 11</title>
        <description>&lt;h1 id=&quot;nesterovs-accelerated-gradient-descent&quot;&gt;Nesterov’s accelerated gradient descent&lt;/h1&gt;
</description>
        <pubDate>Tue, 06 Oct 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2020/10/06/OptLecture11.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2020/10/06/OptLecture11.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Visualizing Data using t-SNE</title>
        <description>&lt;p&gt;2008년&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;abtract&quot;&gt;[Abtract]&lt;/h2&gt;

&lt;p&gt;이 논문에서 제안하는 t-SNE는 Stochastic Neighbor Embedding의 변형으로, 데이터의 여러 다른 스케일의 구조를 잘 표현하는 단일 맵을 만들 수 있는 방법이다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;이전에도 차원을 축소해서 데이터의 관계를 보여주는 연구들은 있었는데, 선형 방법과 비선형 방법이 있다. 차원을 축소하는 방법은 기본적으로 고차원 데이터의 구조를 최대한 보전하는 데에 있다. 선형 방법은 PCA, MDS등의 방법으로, 서로 다른 특성의 데이터를 멀리 표현할 수 있다. 단, 서로 비슷한 특성의 데이터를 가깝게 유지하는 것은 선형 방법으로는 할 수 없고 비선형 방법으로만 할 수 있다. 비선형 방법도 많은 방법들이 연구되었지만, 이들 중 대부분은 지역적 구조와 글로벌 구조를 단일 맵에 다 담지 못한다는 단점이 있다. t-SNE는 이들과 반대로 고차원 데이터의 지역적 구조와 글로벌 구조도 담을 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;2-stochastic-neighbor-embedding-sne&quot;&gt;[2] Stochastic Neighbor Embedding (SNE)&lt;/h2&gt;

&lt;p&gt;SNE는 고차원 데이터의 점 사이 유클리드 거리를 유사도를 나타내기 위한 조건부 확률로 변환하는 것에서 시작한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{j\mid i}=\frac{\exp(-\|x_i-x_j\|^2/2\sigma_i^2)}{\sum_{k\not{=}i}\exp(-\|x_i-x_k\|^2/2\sigma_i^2)}&lt;/script&gt;

&lt;p&gt;저차원 데이터에 대해서는 비슷한 $q_{j|i}$를 계산할 수 있는데, 이때는 &lt;script type=&quot;math/tex&quot;&gt;q_{j\mid i}=\frac{\exp(-\|y_i-y_j\|^2)}{\sum_{k\not{=}i} \exp(-\|y_i-y_k\|^2)}&lt;/script&gt;을 사용한다. 만약 제대로 맵핑이 되었다면, $p_{j|i}$와 $q_{j|i}$는 같을 것이다. 여기서 착안해, SNE에서는 KL-Divergence를 cost function으로 잡고 GD를 이용해 cost를 최소화하는 맵핑을 찾는다. 그러나 KL-Divergence는 대칭이 아니기 때문에, 데이터의 글로벌 구조보다 지역적 구조를 표현하는 것에 더 집중한다. SNE에서 사용하는 cost function은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;C=\sum_i\text{KL}(P_i\|Q_i)=\sum_i\sum_jp_{j\mid i}\log\frac{p_{j\mid i}}{q_{j\mid i}}&lt;/script&gt;

&lt;p&gt;$\sigma_i$를 정하는 일이 남았는데, 모든 점에 대해 고정된 $\sigma_i$가 존재하는 것이 아니고 데이터의 쏠림에 따라 다른 값이 적절하다. SNE는 사용자가 정의한 perplexity ($\text{Perp}(P_i)=2^{H(P_i)}$, $H(P_i)=-\sum_j p_{j|i}\log_2 p_{j|i}$)를 만족하는 $\sigma_i$를 찾는데, 이진 탐색을 사용한다.&lt;/p&gt;

&lt;p&gt;그리고 GD를 이용해 cost를 최소화해야 하는데, 이때의 gradient는 다음과 같이 정의되고, 그 다음 식을 이용해 update된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\delta C}{\delta y_i}=2\sum_j(p_{j\mid i}-q_{j\mid i}+p_{i\mid j}-q_{i\mid j})(y_i-y_j)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{Y}^{(t)}=\mathcal{Y}^{(t-1)}+\eta\frac{\delta C}{\delta\mathcal{Y}}+\alpha(t)(\mathcal{Y}^{(t-1)}-\mathcal{Y}^{(t-2)})&lt;/script&gt;

&lt;p&gt;추가로, 최적화의 초기 단계에서는 반복마다 Gaussian noise를 추가하고, 점차 그 분산을 줄여 simulated annealing의 효과가 나게 한다. 이렇게 하면 나쁜 local minima에 빠지는 것을 막을 수 있다. 그러나 Gaussian noise의 초기값을 선택하는 것과 각 반복마다 얼마나 줄일지에 결과가 민감하게 반응하며, momentum $\alpha(t)$와 step size $\eta$도 선택해야 한다.&lt;/p&gt;

&lt;h2 id=&quot;3-t-distributed-stochastic-neighbor-embedding&quot;&gt;[3] t-Distributed Stochastic Neighbor Embedding&lt;/h2&gt;

&lt;p&gt;SNE는 cost function이 최적화하기 어려운 형태이며, crowding problem이라는 문제가 있다. t-SNE에서 사용하는 cost function은 SNE에서 사용하느 것과 두 가지의 차이점이 있는데, 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;SNE에서 사용하는 cost function의 대칭 버전을 사용하고, gradient도 더 간단하다.&lt;/li&gt;
  &lt;li&gt;저차원 공간에서의 유사도를 계산할 때는 Gaussian noise 대신 Student-t 분포를 사용한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;31-symmetric-sne&quot;&gt;[3.1] Symmetric SNE&lt;/h3&gt;

&lt;p&gt;대칭 SNE에서 사용하는 cost function은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;C=\text{KL}(P\|Q)=\sum_i\sum_jp_{ij}\log\frac{p_{ij}}{q_{ij}}&lt;/script&gt;

&lt;p&gt;이때, $p_{ij}=p_{ji}, q_{ij}=q_{ji}, \forall i,j$의 성질을 갖는다. (위에서는 $\sigma_i$때문에 보통 다르다.) 이때의 $p_{ij}, q_{ij}$는 &lt;script type=&quot;math/tex&quot;&gt;p_{ij}=\frac{p_{j\mid i}+p_{i\mid j}}{2n}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;q_{ij}=\frac{\exp(\|y_i-y_j\|^2)}{\sum_{k\not{=}l}\exp(\|y_k-y_l\|^2)}&lt;/script&gt;로 정의된다. 새로운 gradient는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\delta C}{\delta y_i}=4\sum_j(p_{ij}-q_{ij})(y_i-y_j)&lt;/script&gt;

&lt;p&gt;로, 원래의 gradient보다 훨씬 간단한 모양을 띤다.&lt;/p&gt;

&lt;h3 id=&quot;32-the-crowding-problem&quot;&gt;[3.2] The Crowding Problem&lt;/h3&gt;

&lt;p&gt;2차원 맵의 pairwise distance가 10차원 맵의 pairwise distance를 표현할 수 없는 이유중 하나가 crowding problem인데, 이 문제는 중간 거리의 데이터들을 표현할 수 있는 공간이 가까운 거리의 데이터들을 표현할 수 있는 공간에 비해 충분하지 않다는 문제이다. (10차원의 공간에서는 $r^{10}$이나 되지만, 2차원에서는 $r^2$밖에 없다.)&lt;/p&gt;

&lt;h3 id=&quot;33-mismatched-tails-can-compensate-for-mismatched-dimensionalities&quot;&gt;[3.3] Mismatched Tails can Compensate for Mismatched Dimensionalities&lt;/h3&gt;

&lt;p&gt;crowding problem을 해결하기 위해 t-SNE에서는 거리를 확률로 변환하는 데에 Gaussian 대신 long tail을 가진 Student t 분포를 사용한다. 이를 사용하면 적당히 다른 점들을 적당한 거리로 띄워놓을 수 있다. 이 때 $q_{ij}$의 식은 &lt;script type=&quot;math/tex&quot;&gt;q_{ij}=\frac{(1+\|y_i-y_j\|^2)^{-1}}{\sum_{k\not{=}l}(1+\|y_k-y_l\|^2)^{-1}}&lt;/script&gt;로 바뀐다. 이 방식으로 구한 gradient는 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\delta C}{\delta y_i}=4\sum_j(p_{ij}-q_{ij})(y_i-y_j)(1+\|y_i-y_j\|^2)^{-1}&lt;/script&gt;

&lt;p&gt;고차원 거리와 저차원 거리에 따른 gradient를 그래프로 나타내어 보았는데, t-SNE가 기존 방법보다 나은 점은 두 가지로 볼 수 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;저차원 공간에서 작은 거리로 맵핑되었지만 비슷하지 않은 두 점을 밀어내는 힘이 더 크다.&lt;/li&gt;
  &lt;li&gt;밀어내는 힘이 크지만 무한히 커지진 않는다. (UNI-SNE에서는 무한히 커진다)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;또한 t-SNE의 cost function은 최적화하기가 더 쉬우며, 최적화 초기에 떨어져 있었지만 비슷한 두 점을 끌어당길 수 있는 long-range force가 있다.&lt;/p&gt;

&lt;h3 id=&quot;34-optimization-methods-for-t-sne&quot;&gt;[3.4] Optimization Methods for t-SNE&lt;/h3&gt;

&lt;p&gt;t-SNE에서의 최적화는 두가지의 방법을 통해 더 개선될 수 있다. 하나는 early compression이라고 불리는데, 최적화 초기에 점들이 붙어있도록 한다. 이렇게 하면 점들의 거리가 작아지는데, 클러스터들이 서로를 잘 통과할 수 있게 해주어 데이터의 글로벌 구조를 더 잘 탐색할 수 있게 해준다. 구현은 cost function에 L2-penalty를 추가하는 것으로 할 수 있다.&lt;/p&gt;

&lt;p&gt;다른 하나는 early exaggeration인데, 최적화 초기에 모든 $p_{ij}$에 특정 상수를 곱하는 것이다. 이렇게 하면 원래 값이 작은 $p_{ij}$가 $q_{ij}$와 대등하게 취급되는 것을 도와주어 점들을 멀리 퍼지게 한다. 이렇게 하면 빈 공간이 많아지기 때문에 클러스터들이 좋은 글로벌 구조를 찾을 수 있게 해준다.&lt;/p&gt;

&lt;h2 id=&quot;4-experiments&quot;&gt;[4] Experiments&lt;/h2&gt;

&lt;p&gt;이 논문에서 main paper에 시각화할 데이터셋은 MNIST, Olivetti faces, COIL-20이다. Olivetti faces는 40명의 사람을 10가지의 표정, 안경의 다양성을 준 데이터셋이다. COIL-20은 20개의 대상을 72개의 시점에서 본 데이터셋이다. 각 방법을 비교할 때 우선 PCA를 이용해 30차원까지 줄인 다음 각 방법을 비교한다.&lt;/p&gt;

&lt;h3 id=&quot;43-results&quot;&gt;[4.3] Results&lt;/h3&gt;

&lt;p&gt;우선 MNIST를 가지고 t-SNE, Sammon mapping, Isomap, LLE를 비교했는데, Sammon mapping은 전체적으로 하나의 공 모양을 형성해 클래스를 분리하기 어렵게 보였다. Isomap과 LLE는 여러 클래스 사이에 오버랩이 많았다. 반면 t-SNE는 서로 다른 클래스를 잘 분리했다. t-SNE가 분리한 결과에서 잘못된 클래스로 클러스터링된 결과가 몇개 있는데, 이들은 대부분 숫자가 왜곡되어 사람이 보기에도 다른 클래스처럼 보이는 것들이다.&lt;/p&gt;

&lt;p&gt;COIL-20 데이터셋에서도 t-SNE를 이용했을 때의 결과는 72개의 시점이 원의 모양이 되도록 잘 클러스터링 되어있다.&lt;/p&gt;

&lt;h2 id=&quot;5-applying-t-sne-to-large-data-sets&quot;&gt;[5] Applying t-SNE to Large Data Sets&lt;/h2&gt;

&lt;p&gt;t-SNE는 데이터 개수의 제곱에 해당하는 계산복잡도를 갖고 있기 때문에 그냥 t-SNE를 매우 큰 데이터셋에 적용하면 계산량이 엄청나게 많다. 따라서 랜덤으로 고른 랜드마크만을 t-SNE로 시각화하는데, 전체 데이터셋을 다 이용하는 방법을 사용한다.&lt;/p&gt;

&lt;p&gt;우선 적당한 이웃의 수를 설정하고 이웃 그래프를 그리는 것에서부터 시작한다. 이 방법에서는 $p_{j|i}$를 랜드마크 $x_i$에서 시작해서 랜덤 워크를 수행해 $x_j$에서 끝나는 비율로 정의한다. 이때 노드 $x_a$에서 $x_b$로 가는 길을 선택할 확률은 &lt;script type=&quot;math/tex&quot;&gt;e^{-\|x_a-x_b\|^2}&lt;/script&gt;에 비례한다.&lt;/p&gt;

&lt;p&gt;이 방법이 잘 작동된다는 증거는 일반화 test error에서 찾을 수 있는데, 원래 차원에서의 1-NN test error와 t-SNE를 사용해 축소된 차원에서의 1-NN test error가 거의 비슷했다.&lt;/p&gt;

&lt;h2 id=&quot;6-discussion&quot;&gt;[6] Discussion&lt;/h2&gt;

&lt;h3 id=&quot;61-comparison-with-related-techniques&quot;&gt;[6.1] Comparison with Related Techniques&lt;/h3&gt;

&lt;p&gt;Classical scaling은 PCA와도 연관되어 있는데, 고차원상의 거리와 저차원상의 거리의 SSE를 줄이는 것이 목표인 방법이다. 이 방법의 문제점은 curved manifold모델링에는 성능이 좋지 않으며, 가까이에 있는 점들 사이의 거리보다는 멀리 있는 점들 사이의 거리를 유지하는 데에 중점을 더 둔다. 이 방법을 개선한 방법이 Sammon mapping이다.&lt;/p&gt;

&lt;p&gt;Sammon mapping의 약점은 매우 가까운 고차원의 두 점이 cost function에 큰 영향을 미친다는 점이다. 이 cost function은 분모에 &lt;script type=&quot;math/tex&quot;&gt;\|x_i-x_j\|&lt;/script&gt;가 나눠지기 때문에 매우 가까운 고차원의 두 점이 있으면 cost function값이 엄청나게 커진다.&lt;/p&gt;

&lt;p&gt;Isomap의 단점은 short-circuiting이다. 또한 Isomap은 큰 측지 거리를 모델링하는데에만 초점을 맞춘다. LLE를 사용할 때는 모든 점이 한 점으로 맵핑될 수 있다는 문제가 있는데, 이것을 막아줄 수 있는 공분산 제약이 있다. 문제는 이 제약이 이 문제를 해결하는 쪽이 아닌, 우회하는 방향으로 쉽게 충족될 수 있다는 점이다. 또한 Isomap과 LLE는 이웃 그래프 기반 방법으로 분류되는데, 이들은 여러개의 널리 퍼진 submanifold를 시각화하기 어렵다.&lt;/p&gt;

&lt;p&gt;t-SNE의 랜덤워크 버전도 이웃 그래프를 사용하지만, 모든 경우의 수를 고려하기 때문에 short-circuiting 문제를 피할 수 있다. t-SNE의 랜덤워크 버전은 diffusion map과도 비교될 수 있는데, diffusion map은 classical scaling과 마찬가지의 단점이 있고 또한 $t$라는 하이퍼파라미터를 결정해야 한다.&lt;/p&gt;

&lt;h3 id=&quot;62-weaknesses&quot;&gt;[6.2] Weaknesses&lt;/h3&gt;

&lt;p&gt;t-SNE의 약점은 세가지가 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;t-SNE가 일반적인 차원 축소(2, 3차원이 아닌 더 높은 차원)에 사용될 수 있는지 확실하지 않다. 이 논문에서의 연구는 시각화를 하기 위한 것이기 때문에 2차원으로만 축소했는데, 그 이상 차원으로 축소하면 결과를 평가할 수 없다. 또한 Student t분포는 heavy tail을 갖고있기 때문에 데이터의 local 구조를 잘 보존하지 못하는 결과가 나올 수도 있다.&lt;/li&gt;
  &lt;li&gt;내재된 차원의 저주에 민감할 수도 있다. 만약 데이터를 온전히 표현하기 위한 차원이 매우 큰데 그것을 2차원으로 나타내려 하면 제대로 표현이 안될 수도 있다. t-SNE는 선형성만 가정하기 때문에, 오토인코더 등의 비선형 레이어로 표현된 데이터는 t-SNE로 시각화가 가능하다.&lt;/li&gt;
  &lt;li&gt;cost function이 convex가 아니기 때문에 global optima로 수렴하는 것이 확실하지 않다. classical scaling, Isomap, LLE, diffusion map은 cost function이 convex이다. cost function이 convex하지 않으면 몇몇 최적화 파라미터를 선택해야 한다. 하지만 여러번 실행했을 때 결과가 많이 차이나지 않기 때문에 이 논문에서는 이 점이 큰 약점이 아니라고 주장한다.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 22 Jan 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2020/01/22/tSNE.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2020/01/22/tSNE.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Robust Multi-Agent Reinforcement Learning via Minimax Deep Deterministic Policy Gradient</title>
        <description>&lt;p&gt;2019년&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;이 논문에서는 멀티에이전트 환경에서 상대방의 정책이 바뀌더라도 여전히 일반화에 강인한 deep reinforcement learning (DRL) 에이전트를 만드는 것에 중점을 두었다. 또한 이를 위해 새로운 MiniMax Multi-agent Deep Deterministic Policy Gradient (M3DDPG)를 제안한다. 그리고 제안된 식을 효율적으로 풀기 위해 이들은 Multi-Agent Adversarial Learning (MAAL)을 제안한다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;agent 각각의 RL 훈련은 여러 연구를 통해 연구되었지만, 클래식 단일 에이전트가 multi-agent환경에 놓이게 되면 각 에이전트 입장에서 볼때 환경이 non-stationary가 된다. 이런 환경에서는 에이전트의 수가 늘어남에 따라 policy gradient의 분산이 지수적으로 늘어난다. 기존 방법에서는 이미 학습된 에이전트의 테스트 환경에서 상대가 갑자기 행동을 바꾼다면 해당 에이전트의 policy가 굉장히 나쁜 방법일 수 있다. M3DDPG는 클래식 MADDPG의 minimax 확장 버전이며, 이 알고리즘을 이용하면 상대가 갑자기 행동을 바꾸는 상황에서도 잘 작동할 수 있다. 또한 이 새로운 minimax learning objective를 효율적으로 최적화하기 위한 end-to-end approach인 MAAL도 제안한다.&lt;/p&gt;

&lt;h2 id=&quot;2-related-work&quot;&gt;[2] Related Work&lt;/h2&gt;

&lt;p&gt;이 논문에서의 주요 개념들은 Multi-agent RL, MiniMax, Robust RL이다. Multi-agent RL 개념에서는 기존 MADDPG 알고리즘에서 사용되었던 decentralized policy와 centralized critic framework가 사용된다. 또한 강인한 policy 학습을 위해 minimax를 사용하였다. 제안된 MAAL은 adversarial learning에서 영감을 받았으며, 이는 GD를 사용해 minimax objective를 최소화하는 방법이다.&lt;/p&gt;

&lt;h2 id=&quot;3-background-and-preliminary&quot;&gt;[3] Background and Preliminary&lt;/h2&gt;

&lt;p&gt;RL에서 정의되어야 하는 것은 Q함수, objective, loss이다. 이때 Q함수 $Q(s,a|\theta)$는 $\theta$를 파라미터로 갖는 policy를 따랐을 때 해당 state에서 해당 action을 했을 때 얻을 수 있는 현재 보상 + 예측 보상이고, objective $J(\theta)$는 파라미터를 $
\theta$로 갖는 policy를 따랐을 때의 기대 보상이며, loss $\mathcal{L}(\theta)$는 네트워크에서 줄여야 할 대상으로, 실제 보상과 예측 보상의 MSE이다. $\mathcal{L}$은 Q함수 자체를 학습하는데 사용되고, $J$는 $\theta$를 학습하는데 사용된다.&lt;/p&gt;

&lt;h3 id=&quot;markov-games&quot;&gt;Markov Games&lt;/h3&gt;

&lt;p&gt;Markov Game은 $N$개의 에이전트가 있고, state set $\mathcal{S}$가 있을 때 각 에이전트가 각자의 리워드를 최대로 하는 목표를 가지는 게임이다.&lt;/p&gt;

&lt;h3 id=&quot;q-learning-and-deep-q-networksdqn&quot;&gt;Q-Learning and Deep Q-Networks(DQN)&lt;/h3&gt;

&lt;p&gt;Q-Learning은 policy $\pi$에 대한 action-value 함수를 사용하는데, 이 Q함수는 $t$시간에서 상태가 $s$이고 행동 $a$를 취할 때 얻을 수 있는 리워드의 기댓값($Q^\pi(s, a)=\mathbb{E}[R|s^t=s, a^t=a]$)이다. DQN에서는 loss를 최소화하는 최적의 policy를 찾아 $Q^* $를 학습한다. Q-Learning은 discrete한 action space를 갖는 DRL 에이전트에 가장 적합하다.&lt;/p&gt;

&lt;h3 id=&quot;policy-gradient-pg-algorithms&quot;&gt;Policy Gradient (PG) Algorithms&lt;/h3&gt;

&lt;p&gt;PG의 주 아이디어는 objective $J(\theta)=\mathbb{E}&lt;em&gt;{s\sim\rho^\pi,a\sim\pi&lt;/em&gt;\theta}[R]$ 를 최대화하는 방향의 gradient를 이용해 policy의 파라미터 $\theta$를 바로 찾는 것이다. 이때의 gradient는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown_\theta J(\theta)=\mathbb{E}_{s\sim \rho^\pi,a\sim\pi_\theta}[\triangledown_\theta \log\pi_\theta(a\mid s)Q^\pi(s,a)]&lt;/script&gt;

&lt;p&gt;라고 표현될 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;deterministic-policy-gradient-dpg-algorithms&quot;&gt;Deterministic Policy Gradient (DPG) Algorithms&lt;/h3&gt;

&lt;p&gt;DPG는 PG알고리즘에 deterministic policy $\mu_\theta:\mathcal{S}\rightarrow \mathcal{A}$를 적용한 것이다. 이때 특정 조건 하에서는 gradient를&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown_\theta J(\theta)=\mathbb{E}_{s\sim \mathcal{D}}[\triangledown_\theta \mu_\theta(s)\triangledown_aQ^\mu(s,a)\mid_{a=\mu_\theta(s)}]&lt;/script&gt;

&lt;p&gt;라고 쓸 수 있다. 이때 $\mathcal{D}$는 replay buffer이다. $\triangledown_aQ^\mu(s,a)$가 존재해야 하기 때문에 actio space $\mathcal{A}$는 연속이어야 한다. Deep deterministic policy gradient(DDPG)는 policy $\mu$와 critic $Q^\mu$를 딥러닝 네트워크로 근사하는 방법이다.&lt;/p&gt;

&lt;h3 id=&quot;multi-agent-deep-deterministic-policy-gradient-maddpg&quot;&gt;Multi-Agent Deep Deterministic Policy Gradient (MADDPG)&lt;/h3&gt;

&lt;p&gt;단일 에이전트 RL을 그대로 multi-agent 환경으로 옮기는 것은, 각 agent의 입장에서 수렴에 필요한 Markov assumption을 위배하기 때문에 문제가 된다. 따라서 MADDPG에서는 각 에이전트가 중앙 Q함수를 학습한다. 이때 objective는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown_{\theta_i}J(\theta_i)=\mathbb{E}_{x, a\sim\mathcal{D}}[\triangledown_{\theta_i}\mu_i(o_i)\triangledown_{a_i}Q_i^\mu(x, a_1, \cdots, a_N)\mid_{a_i=\mu_i(o_i)}]&lt;/script&gt;

&lt;p&gt;가 되고, $Q_i^\mu$는 각 에이전트 $i$의 Q값을 알려주는 중앙 Q함수이고, $x$는 state information이다. replay buffer $\mathcal{D}$는 튜플 $(x, x’,a_i,\cdots,a_N,r_1,\cdots,r_N)$을 이용해 모든 에이전트에 대한 경험을 저장한다. 중앙 Q함수는 학습할 때만 사용된다.&lt;/p&gt;

&lt;h2 id=&quot;4-minimax-multi-agent-deep-deterministic-policy-gradient-m3ddpg&quot;&gt;[4] Minimax Multi-Agent Deep Deterministic Policy Gradient (M3DDPG)&lt;/h2&gt;

&lt;h3 id=&quot;minimax-optimization&quot;&gt;Minimax Optimization&lt;/h3&gt;

&lt;p&gt;robust한 policy를 학습하기 위해 이 논문에서는 최악의 상황을 항상 고려하도록 policy를 업데이트 하기로 했다. 모든 다른 에이전트가 반대로 행동한다고 가정하는 것이다. 이 논문에서는 새로운 Q함수를 정의했는데,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_{M,i}^\mu(s, a_1,\cdots,a_N)=r_i(s,a_1,\cdots,a_N)+\gamma\mathbb{E}_{s'}\left[\min_{a_{j\not{=}i}}Q_{M,i}^\mu(s',a_1',\cdots,a_N')]\mid_{a_i'=\mu_i(s')}\right]&lt;/script&gt;

&lt;p&gt;로 쓸 수 있다. 다시 말하면, 현재 리워드와 다음 state $s’$에서 얻는 최악 리워드의 기댓값인데, 이는 policy 없이 $Q_M^\mu$를 업데이트 하는 것을 도와준다.&lt;/p&gt;

&lt;h3 id=&quot;multi-agent-adversarial-learning&quot;&gt;Multi-Agent Adversarial Learning&lt;/h3&gt;

&lt;p&gt;action space $\mathcal{A}$가 연속적이고 Q함수가 비선형이기 때문에 M3DDPG를 최적화하는 것은 매우 많은 계산이 필요하다. 따라서 이 논문에서는 효율적이고 end-to-end인 방법인 MAAL을 제안한다. 이 방법은&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;비선형 Q함수를 부분 선형 함수로 근사한다.&lt;/li&gt;
  &lt;li&gt;최소화의 inner-loop를 한 단계의 GD로 만든다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;위 두 단계를 거친다. 이전 update rule에서 Q함수를 최소화시키는 다른 에이전트의 행동인 $\epsilon$을 이용해 식을 간소화하고, 이를 step size $\alpha$를 갖는 GD로 표현할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;discussion&quot;&gt;Discussion&lt;/h3&gt;

&lt;p&gt;Connection to Adversarial Training - Adversarial Training에서 학습을 빠르게 할 수 있었던 핵심은 loss 함수를 부분 선형 함수로 근사하고, $\epsilon^* $를 scaled gradient로 근사하는 것이었는데, 모든 에이전트의 action을 input으로 받는 중앙 Q함수 덕분에 M3DDPG에서도 같은 방법을 사용하여 빠르게 할 수 있었다.&lt;/p&gt;

&lt;p&gt;Connection to Single Agent Robust RL - 이전에 연구된 robust reinforcement learning (RRL)은 시뮬레이션으로 한 training과 현실세계 testing의 gap을 training 중에 adversarial perturbation $\epsilon$을 넣음으로써 이어주는 방법을 사용했다. MAAL에서는 multi-agent 환경에서 다른 에이전트들의 worst case perturbation $\epsilon$을 넣어주는 방법을 사용했다. 단일 에이전트 입장에서 보면 RRL의 특별 케이스라고 볼 수 있다.&lt;/p&gt;

&lt;p&gt;Choice of $\alpha$ - $\alpha=0$이면 M3DDPG는 MADDPG와 같아진다. $\alpha$가 커지면 더 robust하게 학습할 수 있지만 최적화가 어려워진다. 일정한 $\alpha$를 사용하는 것도 unstable한데, gradient의 scale이 매번 달라지기 때문이다. 이전 연구에서는 gradient를 정규화하는 방법을 사용했는데, M3DDPG에서는 일반 감독학습과는 다르게 input의 scale도 매번 달라지기 때문에 이 논문에서는 정규화한 gradient에 input (action) $a_j$의 norm도 곱해준다.&lt;/p&gt;

&lt;h2 id=&quot;5-experiments&quot;&gt;[5] Experiments&lt;/h2&gt;

&lt;p&gt;이 논문에서는 $N$개의 협력 에이전트와 $M$개의 대적 에이전트, 그리고 $L$개의 랜드마크로 구성된 환경으로 실험한다. 서로 다른 네개의 환경에서 실험을 진행한다.&lt;/p&gt;

&lt;h3 id=&quot;comparison-to-maddpg&quot;&gt;Comparison to MADDPG&lt;/h3&gt;

&lt;p&gt;실험에서는 $0\sim1$로 정규화한 리워드를 평가 지표로 삼았는데, M3DDPG가 협력 에이전트이고 MADDPG가 대적 에이전트일 때가 점수가 가장 높았고, MADDPG가 협력 에이전트이고 M3DDPG가 대적 에이전트일 때가 점수가 가장 낮았다. 이로써 M3DDPG가 MADDPG보다 더 좋은 성능을 가졌음을 확인할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;evaluation-with-disruptive-adversaries&quot;&gt;Evaluation with Disruptive Adversaries&lt;/h3&gt;

&lt;p&gt;이들을 서로 대적시키면 서로가 상대의 눈치를 보며 내 리워드를 최대로 하는 행동만을 하기 때문에 최악의 행동만을 하는 대적 에이전트의 상황도 봐야 한다. 이 논문에서는 문제를 제로섬으로 바꾸고 대적 에이전트를 DDPG를 이용해 협력 에이전트의 반대 리워드로 학습시켰다. 이 상황에서도 M3DDPG가 MADDPG보다 나은 성능을 보였다.&lt;/p&gt;

&lt;h2 id=&quot;6-conclusion&quot;&gt;[6] Conclusion&lt;/h2&gt;

&lt;p&gt;많은 장점들에도 불구하고 M3DDPG에는 단점이 있는데, 계산량을 줄이려고 MAAL에서 한 단계의 GD만을 사용하기 때문에 지역적인 worst 상황만을 고려한다.&lt;/p&gt;
</description>
        <pubDate>Mon, 20 Jan 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2020/01/20/RobustRL.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2020/01/20/RobustRL.html</guid>
        
        <category>ReinforcementLearning</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
        <description>&lt;p&gt;2015년&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;딥러닝 네트워크는 각 layer의 input의 분포가 학습 도중 계속 바뀌기 때문에 어려워진다. 이를 internal covariate shift라고 하는데, 이 논문에서는 이 문제를 layer input을 정규화하므로써 해결했다. 이렇게 하면 더 큰 lr을 사용할 수 있고, 초기화에 덜 민감해진다. 또한 regularizer의 역할을 해서 Dropout등이 필요없게 된다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;SGD는 input의 작은 변화가 모델 파라미터 전체에 끼치는 영향이 확대되기 때문에 lr같은 하이퍼파라미터 튜닝에 굉장히 민감하다. 모델의 input 분포가 학습 도중 계속 바뀌는 현상을 covariate shift라고 하는데 전형적으로 domain adaptation에 의해 해결되었다. 그런데 네트워크는 각 layer를 하나의 네트워크라고 볼 수 있기 때문에 네트워크 전체의 학습을 잘 시킬 수 있는 방법(input의 분포를 고정하는 방법)을 각 layer마다 적용할 수도 있다.&lt;/p&gt;

&lt;p&gt;만약 네트워크에서 sigmoid함수를 활성함수로 사용한다면, input의 절댓값이 커지면 기울기는 0으로 갈 것이고, 이 말은 작은 몇몇 input값에서 말고는 업데이트가 느려진다. 이런 현상은 네트워크 전체에서 확대된다. 사실 이 문제는 ReLU를 이용하여 많이 없어지기는 했지만 여전히 존재하며, input의 분산이 학습 중에 계속해서 고정된다면 학습이 더 가속될 것이다.&lt;/p&gt;

&lt;p&gt;각 layer의 input 분포가 바뀌는 것을 internal covariate shift라고 하는데, 이를 없애면 확실히 학습을 빨리 할 수 있다. 이 논문에서는 batch normalization이라는 방법을 제안하는데, 이 방법은 layer input의 평균과 분산을 고정시켜주는 방법이다. 이 방법을 사용하면 gradient의 파라미터 스케일이나 초기값에 대한 의존을 줄여주고, 따라서 더 큰 lr을 사용할 수 있게 해준다. 따라서 학습을 더 빠르게 만들어준다.&lt;/p&gt;

&lt;h2 id=&quot;2-towards-reducing-internal-covariate-shift&quot;&gt;[2] Towards Reducing Internal Covariate Shift&lt;/h2&gt;

&lt;p&gt;이전 연구에서 network의 input이 whitening되면 수렴이 빨라진다는 연구가 있었기 때문에 internal covariate shift를 없앰으로써 수렴속도가 빨라지는 것을 기대했다. 하지만 매 input마다  normalization만 하면 gradient descent가 진행될 때 network의 파라미터가 무시되는 현상이 나타났다. 따라서 dataset 전체의 평균과 분산을 가지고 각 input들을 normalization한다.&lt;/p&gt;

&lt;h2 id=&quot;3-normalization-via-mini-batch-statistics&quot;&gt;[3] Normalization via Mini-Batch Statistics&lt;/h2&gt;

&lt;p&gt;normalization은 전체 layer의 표현력을 바꿀 수 있기 때문에, normalization후에 학습가능한 파라미터($\gamma, \beta$)로 다시 선형 변환을 해주는 부분을 넣는다. 또한 SGD에서 사용하기 위해서 dataset 전체의 평균과 분산이 아닌 각 minibatch의 평균과 분산을 이용해 normalization한다.&lt;/p&gt;

&lt;h3 id=&quot;31-training-and-inference-with-batch-normalized-networks&quot;&gt;[3.1] Training and Inference with Batch-Normalized Networks&lt;/h3&gt;

&lt;p&gt;training에서는 위와 같은 방법으로 파라미터를 훈련시키고, testing에서는 normalization과 $\gamma, \beta$를 한 과정으로 압축한다. 또한 testing에서는 batch 평균과 분산이 없으므로, train data에서의 전체 평균과 분산을 사용하는데, 메모리의 제약 때문에 moving average ($\hat{\mu}\leftarrow \alpha\hat{\mu}+(1-\alpha)\mu$)를 사용한다.&lt;/p&gt;

&lt;h3 id=&quot;32-batch-normalized-convolutional-networks&quot;&gt;[3.2] Batch-Normalized Convolutional Networks&lt;/h3&gt;

&lt;p&gt;CNN에서는 BN을 적용하는 방식이 조금 달라지는데, feature map별로 서로 다른 파라미터를 적용한다. 또한 minibatch에서 같은 feature map에 있는 input을 묶어서 normalization한다.&lt;/p&gt;

&lt;h3 id=&quot;33-batch-normalization-enables-higher-learning-rates&quot;&gt;[3.3] Batch Normalization enables higher learning rates&lt;/h3&gt;

&lt;p&gt;BN을 사용하면 backpropagation이 파라미터의 scale에 영향을 받지 않는다. 그리고 이 논문에서는 BN이 layer의 Jacobian의 eigenvalue들이 1에 가깝게 되도록 한다고 추측한다. 이렇게 되면 학습에 더 도움이 되지만 진짜 저렇게 되는지는 확인된 바가 없다.&lt;/p&gt;

&lt;h3 id=&quot;34-batch-normalization-regularizes-the-model&quot;&gt;[3.4] Batch Normalization regularizes the model&lt;/h3&gt;

&lt;p&gt;BN을 사용하면 비슷한 대상에 대해 비슷한 input 분포가 나타나기 때문에 해당 input에 대해 새롭게 파라미터를 변경할 필요가 없다. 따라서 BN은 일반화에도 도움을 준다.&lt;/p&gt;

&lt;h2 id=&quot;4-experiments&quot;&gt;[4] Experiments&lt;/h2&gt;

&lt;h3 id=&quot;41-activations-over-time&quot;&gt;[4.1] Activations over time&lt;/h3&gt;

&lt;p&gt;이 실험에서는 MNIST를 썼는데, sota 결과를 달성하는 것이 아닌 baseline과의 비교에 중점을 두었다. BN을 쓴 모델이 test 정확도가 더 높았고, 초반부터 높은 정확도를 보여준다. 또한 각 sigmoid로 들어가는 input의 분포가 BN을 사용하기 전에는 변동이 컸으나 BN을 사용한 것은 변동이 적었다.&lt;/p&gt;

&lt;h3 id=&quot;42-imagenet-classification&quot;&gt;[4.2] ImageNet classification&lt;/h3&gt;

&lt;p&gt;이 실험을 위해서는 변형된 Inception network를 사용하였다. 또한, BN을 그대로 적용하기보다는 성능 개선을 위해 여러 요소를 변경하였다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;lr을 높인다 - BN을 사용하면 더 큰 lr을 사용할 수 있으므로&lt;/li&gt;
  &lt;li&gt;dropout을 없앤다 - BN을 사용하면 일반화가 더 잘되기 때문에 굳이 필요없으므로&lt;/li&gt;
  &lt;li&gt;$L_2$ 정규화 비중을 줄인다 - 실험적으로 이렇게 하면 더 좋은 결과를 얻을 수 있으므로&lt;/li&gt;
  &lt;li&gt;lr decay를 빠르게 한다 - 학습이 빨라지기 때문에 lr도 더 빠르게 줄어야 하므로&lt;/li&gt;
  &lt;li&gt;Local Response Normalization을 없앤다 - BN이 더 좋은 normalization을 해주므로&lt;/li&gt;
  &lt;li&gt;input을 더 철저히 섞는다 - 더 랜덤한 batch들을 사용해야 일반화가 더 잘되므로&lt;/li&gt;
  &lt;li&gt;photometric distortion을 없앤다 - 학습이 더 빠르게 되기 때문에 데이터를 더 적게 보게 되므로&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;single-Network 분류를 사용해본 결과, 같은 정확도에 이르기까지가 BN을 사용한 방법이 훨씬 적게 걸림을 알 수 있었다. 또한 lr을 5배로 높이면 이 속도는 더 빨라지는데, 30배로 높이면 좀 더 느려지는 대신 더 큰 최종 test 정확도를 얻을 수 있다. 또한 BN이 없으면 ReLU 대신 sigmoid를 사용했을 때 학습이 불가능하지만, BN을 사용하면 sigmoid로도 왠만한 정확도를 낼 수 있다. (이전 다른 모델들보다는 낮은 정확도지만)&lt;/p&gt;

&lt;p&gt;ImageNet 경연에서 좋은 결과를 얻은 모델들은 거의다 ensemble을 사용했기 때문에 BN을 이용해서도 ensemble을 사용해봤는데, sota 결과를 갱신했다.&lt;/p&gt;
</description>
        <pubDate>Wed, 15 Jan 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2020/01/15/BatchNorm.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2020/01/15/BatchNorm.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Cyclical Learning Rates for Training Neural Networks</title>
        <description>&lt;p&gt;2017년&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;이 논문에서는 cyclical learning rate를 제시하는데, 이 방법은 적당한 boundary값 사이를 왔다갔다 한다. 이 적당한 boundary를 찾는 방법 또한 제시하는데, learning rate을 몇 epoch동안 계속 늘려보는 방법으로 찾을 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;이 논문에서는 훈련 중에 learning rate을 계속 바꾸는 것이 훈련에 이득이라는 것을 증명했다. 또한 CLR을 이용하면 좋은 성능을 위해서 lr을 튜닝하는 작업도 없앨 수 있다. 그리고 adaptive lr을 사용하면 들여야 하는 추가 계산도 필요하지 않다.&lt;/p&gt;

&lt;h2 id=&quot;2-related-work&quot;&gt;[2] Related work&lt;/h2&gt;

&lt;h3 id=&quot;adaptive-learning-rates&quot;&gt;Adaptive learning rates&lt;/h3&gt;

&lt;p&gt;adaptive lr은 CLR의 경쟁자로 볼 수 있다. adaptive lr은 global lr대신 local adaptive lr에 의존하는 대신, 많은 계산량이 필요하다.&lt;/p&gt;

&lt;p&gt;AdaGrad는 gradient로부터 lr을 예측하는 초기 adaptive lr 방법이다. RMSProp은 해당 weight에 사용되었던 최근 gradient를 평균내서 local adaptive lr을 계산하는 방법이다. 이후에는 gradient의 Hessian의 대각성분을 추정하는 방법이 제안되었으며, 해당 논문에서는 또한 lr을 크게 하는 것이 유의미하다는 것을 보였다. AdaDelta는 AdaGrad를 개선시킨 방법이다. 더 최근에는 AdaSecant방법이 제안되었는데, 이는 gradient의 root mean square 통계와 분산을 사용한 방법이다. ESGD는 RMSProp이 편향된 추정을 제공한다는 점을 개선한 방법이다. 이후 AdaGrad와 RMSProp을 합친 방법인 Adam이 나왔다.&lt;/p&gt;

&lt;p&gt;CLR과 adaptive lr은 완전히 다른 정책을 사용하고, 또한 둘은 합쳐질 수도 있다. CLR은 SGDR과 비슷하기도 하다.&lt;/p&gt;

&lt;h2 id=&quot;3-optimal-learning-rates&quot;&gt;[3] Optimal Learning Rates&lt;/h2&gt;

&lt;h3 id=&quot;31-cyclical-learning-rates&quot;&gt;[3.1] Cyclical Learning Rates&lt;/h3&gt;

&lt;p&gt;CLR의 핵심은 lr을 크게 하는 것이 단기로 보면 좋지 않지만 장기적으로는 이득이 된다는 것이다. 최댓값과 최솟값만 정해놓고 그 사이를 왔다갔다 하는 여러 함수로 실험해 보았는데 최종적으로는 삼각형 모양의 linear schedule을 선택했다. 이유는 이 아이디어를 따르는 가장 단순한 모양이기 때문이다.&lt;/p&gt;

&lt;p&gt;이전의 한 논문에서 훈련을 어렵게 하는 이유 중 하나가 안장점 때문이라는 연구가 있었는데, 안장점에서는 gradient가 작으므로 lr을 높임으로써 학습을 빠르게 할 수 있다. 따라서 CLR이 잘 작동하는 것이다. 또한, 최적의 lr은 bound 안에 있을 것이고 최적에 가까운 lr이 학습 중에 사용되기 때문에 잘 작동하는 것이라는 이유도 있다.&lt;/p&gt;

&lt;p&gt;triangular 방법 말고도 다른 두가지의 방법이 이 논문에서 논의된다. 하나는 triangular2 방법으로, triangular과 거의 비슷하지만 각 cycle이 끝날 때마다 lr의 차이가 반으로 줄어드는 방법이다. exp_range는 각 boundary가 $\gamma^{\text{iteration}}$만큼씩 줄어드는 방법이다.&lt;/p&gt;

&lt;h3 id=&quot;32-how-can-one-estimate-a-good-value-for-the-cycle-length&quot;&gt;[3.2] How can one estimate a good value for the cycle length?&lt;/h3&gt;

&lt;p&gt;실험상 stepsize의 2~8epoch로 하면 좋았다.&lt;/p&gt;

&lt;h3 id=&quot;33-how-can-one-estimate-reasonable-minimum-and-maximum-boundary-values&quot;&gt;[3.3] How can one estimate reasonable minimum and maximum boundary values?&lt;/h3&gt;

&lt;p&gt;최적의 bound를 찾으려면 몇 epoch를 lr을 크게 하면서 돌려보면 된다. 정확도가 증가하는 lr과 감소하는 lr을 각각 하한과 상한으로 잡으면 된다. 이 방법은 최적의 LR과 최적의 LR범위를 알려준다. &lt;em&gt;왜그런지는 모르겠음&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-experiments&quot;&gt;[4] Experiments&lt;/h2&gt;

&lt;h3 id=&quot;41-cifar-10-and-cifar-100&quot;&gt;[4.1] CIFAR-10 and CIFAR-100&lt;/h3&gt;

&lt;p&gt;우선 CIFAR-10 데이터셋을 가지고 실험을 했는데, triangular2 방법을 가지고 baseline의 정확도와 같은 정확도(81.4%)를 달성하는 데에 반복이 훨씬 덜 소요됐다.&lt;/p&gt;

&lt;p&gt;CLR이 잘되는 이유를 lr이 감소하는 부분 때문이라고 생각하고 감소하기만 하는 schedule로 실험을 해봤는데, 결과는 증가하는 부분과 감소하는 부분 둘 다가 있어야 더 좋은 성능을 갖는다는 것을 보였다. 마찬가지로, Caffe에 구현되어 있는 exp와 exp_range를 비교했는데 exp_range의 성능이 더 좋았다. 이 또한 진동하는 lr이 더 효과가 있음을 보여준다. CLR은 또한 batch normalization과도 결합될 수 있는데, 이때도 CLR을 적용한 것이 결과가 더 좋았다.&lt;/p&gt;

&lt;p&gt;이후에는 여러 다른 모델을 사용해서 CIFAR-10과 CIFAR-100에 실험했는데, ResNet, Stochastic Depth network (SD), DenseNet을 비교했다. 이들 모두 CLR을 사용해서 결과가 더 좋아지거나 적어도 비슷한 결과가 나왔다. 따라서 CLR을 사용하는 것이 효과가 있다는 것을 보여준다.&lt;/p&gt;

&lt;h3 id=&quot;42-imagenet&quot;&gt;[4.2] ImageNet&lt;/h3&gt;

&lt;p&gt;이 실험에서는 Caffe에서 제공하는 AlexNet을 baseline으로 잡고 실험했다. CLR을 사용한 것과 사용하지 않은 것이 별로 차이가 없ㅇ었는데, 이를 통해 원래의 baseline lr이 잘 정해졌다는 것을 알 수 있다. GoogleNet에서도 마찬가지로 CLR을 사용한 것이 조금 더 성능이 좋았다.&lt;/p&gt;
</description>
        <pubDate>Tue, 14 Jan 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2020/01/14/CyclicLR.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2020/01/14/CyclicLR.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Automatically Inferring Data Quality for spatiotemporal Forecasting</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;시공간 데이터가 많이 쓰이고 있는데, 이때 데이터의 질이 다양할 수 있다. 이는 잘못하면 신뢰도가 떨어지는 예측으로 이어질 수 있고, 이는 블랙박스인 딥러닝에서는 치명적일 수 있다. 따라서 이 논문에서는 데이터의 질을 자동으로 알려주는 해결책을 제시한다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;여러 다른 센서의 사용으로 데이터에 다양한 노이즈가 낄 수 있고, 따라서 네트워크의 성능 저하가 생길 수 있다. 이 논문에서는 데이터의 질을 예측하는 DQ-LSTM 모델을 제시하는데, 이 모델은 data quality level에 대한 공간적 구조를 잘 탐색할 수 있고, 각 시계열에 대한 의존도도 파악할 수 있다. 관련 연구는 서로 다른 출처에서의 데이터의 질에 대한 연구와, 그래프의 신호를 학습하는 연구가 있었다. 데이터의 질에 관한 연구들은 대부분 label이 필요했는데, 이 논문의 방법에서는 label을 사용하지 않고 서로 다른 출처의 데이터를 구분할 수 있다. 그래프의 신호를 학습하는 연구에서는 특별히 CNN에 관한 연구가 있었는데 이는 지역화된 패턴들을 뽑을 수 있는 구조이다. 이 논문에서는 graph convolution layer를 사용하여 시공간 특징을 data quality에 맵핑하는 모델을 제안한다.&lt;/p&gt;

&lt;h2 id=&quot;2-preliminaries&quot;&gt;[2] Preliminaries&lt;/h2&gt;

&lt;h3 id=&quot;21-local-variation&quot;&gt;[2.1] Local Variation&lt;/h3&gt;

&lt;p&gt;local variation은 원래 $\triangledown_ix=(\frac{\partial x}{\partial e}|_ {e=(i,j)}|j\in \mathcal{N}_i)$라고 정의한다. 이때 $\mathcal{N}_i$는 $i$번째 vertex에 연결된 vertex들인데, vertex마다 연결된 개수가 다르면 차원이 일정하지 않을 수 있다. 따라서 최종적으로&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|\triangledown_ix \|_ 2^2=\sum_{j\in\mathcal{N}_i}W_{ij}(x(j)-x(i))^2&lt;/script&gt;

&lt;p&gt;라고 정의된다. 이 정의에 따르면, 연결된 vertex의 $x$값 (시그널)들이 현재 vertex의 $x$값과 비슷하면 local variation은 작아진다. 또한, local variation은 $W$와 $x$의 함수라는 것도 알 수 있다.&lt;/p&gt;

&lt;p&gt;또한 $M$개의 서로 다른 센서들에서 온 신호가 있다고 할 때, 한 vertex의 local variation을&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_i=(\|\triangledown_ix^1\|_ 2^2,\cdots,\|\triangledown_ix^m\|_ 2^2, \cdots, \|\triangledown_ix^M\|_ 2^2)&lt;/script&gt;

&lt;p&gt;로 나타낼 수 있다. 최종적으로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(W, X)=(D+W)(X\odot X)-2(X\odot WX)&lt;/script&gt;

&lt;p&gt;라고 할 수 있다. ($\odot$은 element-wise multiplication)&lt;/p&gt;

&lt;h3 id=&quot;22-data-quality-level&quot;&gt;[2.2] Data Quality Level&lt;/h3&gt;

&lt;p&gt;data quality level을 정할 때는 주위 신호와 현재 node의 신호가 큰 차이가 없는 것이 바람직하다고 가정한다. data quality level은 $s_i=q(L_i)$라고 할 수 있는데, 함수 $q$는 임의로 정할 수 있다. GCN이라는 그래프 파라미터를 학습할 수 있는 CNN 모델이 제안되었는데, 이 모델을 여러겹 쌓은 모델을 만들어 통과시킨 후 data quality level을 결정할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Z=\sigma_K(\hat{A}\sigma_{K-1}(\hat{A}\cdots\sigma_1(\hat{A}X\Theta_1)\cdots\Theta_{K-1})\Theta_K)&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;s=\sigma_L(L(W,Z)\Phi)&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-model&quot;&gt;[3] Model&lt;/h2&gt;

&lt;h3 id=&quot;data-quality-network&quot;&gt;Data quality network&lt;/h3&gt;

&lt;p&gt;다층의 GCN이 그래프 신호와 DQL 사이에서 연관 신호를 추출하고 필터 크기를 늘릴 수 있다. 이 DQN으로 단일층 뉴럴 네트워크 $\Phi$와 활성함수 $\sigma_L$을 사용한다.&lt;/p&gt;

&lt;h3 id=&quot;long-short-term-memory&quot;&gt;Long short term memory&lt;/h3&gt;

&lt;p&gt;순간적인 신호를 처리하기 위해 이 논문에서는 LSTM을 사용했다. 길이가 $k$인 순차 신호를 LSTM에 넣고 다음 신호를 예측한다. 예측된 신호는 실제 신호와 비교되며 LSTM의 파라미터가 업데이트된다.&lt;/p&gt;

&lt;h3 id=&quot;dq-lstm&quot;&gt;DQ-LSTM&lt;/h3&gt;

&lt;p&gt;LSTM에 신호를 넣기 위해서는 전체 신호를 길이가 $k$인 신호로 세그먼트를 만들어야 한다. 그리고 모든 vertex에 대한 마지막 신호는 GCN의 입력이 된다. loss function은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}_i=\frac{1}{n_i}\sum_{j=1}^{n_i}s_i\|\hat{\mathcal{X}}(i,:,k+j-1)-\mathcal{X}(i,:,k+j-1)\|_2^2+\beta\|\Phi\|_2^2&lt;/script&gt;

&lt;p&gt;$i$는 vertex의 인덱스이고, $j$는 세그먼트의 시작점, $k$는 세그먼트의 길이, $\hat{\mathcal{X}}$는 예측신호값, $\mathcal{X}$는 실제 신호, $s_i$는 노드의 quality level이다.&lt;/p&gt;

&lt;h2 id=&quot;4-experiments&quot;&gt;[4] Experiments&lt;/h2&gt;

&lt;p&gt;이 논문에서는 평가 척도로 mean absolute error (MAE) $\frac{\sum_{i=1}^n|y_i-x_i|}{n}$을 사용했다.&lt;/p&gt;

&lt;h3 id=&quot;42-graph-generation&quot;&gt;[4.2] Graph Generation&lt;/h3&gt;

&lt;p&gt;weather station에 대해서 그래프를 만들어야 하는데, 문제는 어디를 연결해야 하는지 정하는 것이다. 처음에는 거리 기반으로 연결을 정의하려 했으나, 데이터에 거리 외에도 많은 특징들이 있기 때문에 거리가 가까워도 다른 특징들이 많이 차이가 나는 현상이 생길 수 있다. 따라서 이 논문에서는 모든 특징에 똑같은 중요도를 두고 연결을 정의했다.&lt;/p&gt;

&lt;h3 id=&quot;43-baselines&quot;&gt;[4.3] Baselines&lt;/h3&gt;

&lt;p&gt;이 논문에서는 우선 확률론적 방법인 autoregressive와 비교하고, simple LSTM과 비교하고, 마지막으로 GCN과 비교한다.&lt;/p&gt;

&lt;h2 id=&quot;5-results-and-discussion&quot;&gt;[5] Results and Discussion&lt;/h2&gt;

&lt;h3 id=&quot;51-forecasting-experiments&quot;&gt;[5.1] Forecasting Experiments&lt;/h3&gt;

&lt;p&gt;이 논문에서의 결과에 따르면, $K$가 더 크면 더 큰 주변의 신호를 알게 되므로 결과가 더 좋아진다. GCN과 DQ-LSTM의 차이점은, GCN은 주어진 신호에서 data quality를 바로 예측하지만 DQ-LSTM은 local variation을 계산한 후 data quality를 예측한다. DQ-LSTM의 성능이 더 좋은 것으로 볼 때, local variation이 data quality 예측에 유용하게 쓰인다는 것을 알 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;52-node-embedding-and-low-quality-detection&quot;&gt;[5.2] Node Embedding and Low-Quality Detection&lt;/h3&gt;

&lt;p&gt;DQ-LSTM이 GCN과 연결되어 있으므로 GCN에서 나온 output을 가지고 embedding으로 표현할 수 있다. embedding은 낮은 차원을 갖고 있기 때문에 t-SNE와 같은 방법으로 시각화할 수 있다. 이 논문에서는 이 방법을 사용해 시각화했는데, 맨 처음에는 기준 노드에 연결된 노드들이 근처에 있다가, 한 노드가 자신과 연결된 다른 노드에 의해 영향을 더 받게 되면 기준 노드로부터 멀어진다.&lt;/p&gt;

&lt;p&gt;위에서 기준 노드로부터 멀어진 노드를 $F$라고 하고, 그와 연결된 다른 노드를 $C$라고 하자. $F$가 기준 노드와 멀어진 이유는 두 가지로 생각해볼 수 있는데, 하나는 $F$의 신호가 기준 노드에 연결된 다른 노드들의 신호와 매우 다른 것이고, 다른 하나는 $C$의 신호에 노이즈가 많아서 $F$에 영향을 준 것이다. 첫번째 이유는 이전의 가정에 위배되므로 사실이 아니고, 따라서 $C$와 $F$가 low-quality node의 후보가 될 수 있다.&lt;/p&gt;
</description>
        <pubDate>Fri, 10 Jan 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2020/01/10/DataQual.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2020/01/10/DataQual.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Visualizing the Loss Landscape of Neural Nets</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;이 논문에서는 왜 특정 네트워크 구조가 학습이 잘되고 일반화가 잘되는지를 알기 위해, loss function의 구조와 지형을 시각화해보고자 한다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;이 논문에서 알아보고자 하는 것은&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;loss function의 특징&lt;/li&gt;
  &lt;li&gt;네트워크 구조가 loss지형에 미치는 영향&lt;/li&gt;
  &lt;li&gt;non-convex 구조가 학습에 미치는 영향&lt;/li&gt;
  &lt;li&gt;loss function의 지형이 일반화 특성에 미치는 영향&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이다. 그리고 이들은 filter normalization이라는 방법을 사용했다.&lt;/p&gt;

&lt;h2 id=&quot;2-theoretical-background&quot;&gt;[2] Theoretical Background&lt;/h2&gt;

&lt;p&gt;연구자들은 sharpness나 flatness가 일반화 능력에 관련있다고 했다. 그래서 flatness를 최소값 근처의 면적, hessian의 eigenvalue 또는 sharpness를 local entropy를 이용해서 나타내기도 했으나, &lt;a href=&quot;https://arxiv.org/abs/1706.08947&quot;&gt;다른 연구&lt;/a&gt;에서는 일반화 능력은 이런 flatness의 측정값과는 관련이 없다고도 했다.&lt;/p&gt;

&lt;h2 id=&quot;3-the-basics-of-loss-function-visualization&quot;&gt;[3] The Basics of Loss Function Visualization&lt;/h2&gt;

&lt;p&gt;loss function을 시각화하는 데에는 1차원 방법과 2차원 방법이 있다. 1차원 방법은 두 점을 찍어서 그 두 점을 잇는 점들의 loss 값을 그래프로 그리는 방법인데, non-convexity를 시각화하기 어렵고, batch normalization이나 invariance symmetry(또는 scale invariance)를 고려하지 않는다. 2차원 방법은 한 점과 특정 두 방향을 정해서 그 길을 따라가며 2차원으로 그래프를 그리는 방법인데, 계산량이 많아서 resolution이 낮아 역시 non-convexity를 시각화하기 어렵다.&lt;/p&gt;

&lt;h2 id=&quot;4-proposed-visualization--filter-wise-normalization&quot;&gt;[4] Proposed Visualization : Filter-Wise Normalization&lt;/h2&gt;

&lt;p&gt;network weight에는 scale invariance라는 속성이 있는데, 이는 weight에 특정 수를 곱하거나 나누어도 전체 네트워크의 행동이 변하지 않는다는 것이다. 이런 특성 때문에 weight값이 한 unit이 바뀌어도, 원래의 scale이 컸으면 영향이 거의 없고, 원래의 scale이 작았으면 엄청난 영향이 된다. 따라서 여러 다른 minimizer나 다른 네트워크를 비교할 수 없게 된다. 이런 문제를 해결하기 위해 이 논문에서는 filter-wise normalization을 제안하는데, 위의 2차원 방법을 사용하되 두 방향을 고른 뒤 &lt;script type=&quot;math/tex&quot;&gt;d_{i,j}\leftarrow \frac{d_{i,j}}{\|d_{i,j}\|}\|\theta_{i,j}\|&lt;/script&gt;로 normalize하는 방법이다. 이렇게 하면 본래의 거리 scale을 유지할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;5-the-sharp-vs-flat-dilemma&quot;&gt;[5] The Sharp vs. Flat Dilemma&lt;/h2&gt;

&lt;p&gt;1차원 방법으로 small batch의 minimizer $\theta^s$와 large batch의 minimizer $\theta^l$을 그려본 결과, $\theta^s$가 더 flat했고, 실제로 small batch를 사용하면 일반화가 더 잘된다. 하지만 weight decay를 적용하였더니 $\theta^l$이 더 flat했다. 사실 $\theta^s$의 weight들이 크기가 더 크다. 따라서 scale variance때문에 더 flat해보이기만 한 것이다. 마찬가지로 weight decay에서는 shrinkage 효과때문에 $\theta^s$의 weight 크기가 작아졌고, 따라서 $\theta^l$이 더 flat해보인 것이다. 따라서 sharpness와 일반화는 관련이 없다고 결론지을 수 있다.&lt;/p&gt;

&lt;p&gt;이 실험을 filter-wise normalization을 이용해서 다시 해봤는데, 비교해본 결과 weight decay를 사용한 것과 사용하지 않은 것에서 둘다 $\theta^s$이 더 flat했다. 따라서, 사실은 일반화는 sharpness와 관련이 있었지만, 여태까지의 시각화가 잘못된 것임을 알 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;6-what-makes-neural-networks-trainable-insights-on-the-nonconvexity-structure-of-loss-surfaces&quot;&gt;[6] What Makes Neural Networks Trainable? Insights on the (Non)Convexity Structure of Loss Surfaces&lt;/h2&gt;

&lt;p&gt;ResNet과, ResNet과 같은 구조인데 skip connection이 없는 네트워크를 filter-wise normalization을 이용해 비교해본 결과 관찰된 특성이 몇가지 있다.&lt;/p&gt;

&lt;h3 id=&quot;1-네트워크-깊이의-효과&quot;&gt;1. 네트워크 깊이의 효과&lt;/h3&gt;

&lt;p&gt;ResNet-20 with no skip connection은 convex한 모습을 띠는데, 이는 VGG-19와 비슷한 구조로, 원래 학습이 잘되는 구조이다. 그런데 ResNet-56와 ResNet-110 with no skip connection은 복잡하고 non-convex한 모양을 띤다. 반대로 그냥 ResNet-56과 ResNet-110은 여전히 convex한 모양을 띤다. 이것으로, 얕은 네트워크에서는 skip connection의 효용이 두드러지지 않지만, 깊은 네트워크에서는 매우 중요한 역할을 한다고 볼 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;2-wide-model-vs-thin-model&quot;&gt;2. Wide model vs. Thin model&lt;/h3&gt;

&lt;p&gt;WRN의 k를 늘려가며 실험했는데, 더 wide한 네트워크가 더 flat했다. skip connection이 없는 모델에서도 더 넓은 모델이 더 flat한 지형을 갖는다. 또한, 더 wide한 모델의 테스트 정확도가 높았으므로, flatness와 일반화의 관계를 다시한번 확인할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;3-네트워크-초기화&quot;&gt;3. 네트워크 초기화&lt;/h3&gt;

&lt;p&gt;convex에 가까운 ResNet 네트워크나 VGG 네트워크의 경우, 랜덤하게 초기화해도 잘 최적화될 가능성이 높지만, 깊은 ResNet with no skip connection 네트워크의 경우 랜덤 초기화는 gradient가 전혀 정보를 주지 않는 곳에 있을 확률이 높아 학습하기 어렵다.&lt;/p&gt;

&lt;h3 id=&quot;4-convexity&quot;&gt;4. Convexity&lt;/h3&gt;

&lt;p&gt;우리가 본 컨투어는 차원을 엄청나게 낮춘 것이기 때문에, non-convex처럼 보이면 실제 차원에서도 non-convex하지만, convex처럼 보인다고 해서 실제 차원에서 convex인 것은 아니고 양의 curvature들이 우세하다는 정도로만 보아야 한다. 이것을 보이기 위해 이전에 그린 그래프를 따라 &lt;script type=&quot;math/tex&quot;&gt;\|\frac{\lambda_{min}}{\lambda_{max}}\|&lt;/script&gt;를 그려보았는데, convex같아보이는 그래프에서는 이 값들이 작았고, 아닌 그래프에서는 non-convex로 보이는 부분들에 이 값이 크게 나타났다.&lt;/p&gt;
</description>
        <pubDate>Tue, 07 Jan 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2020/01/07/Visual.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2020/01/07/Visual.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>On the Variance of the Adaptive Learning Rate and Beyond</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;learning rate warmup은 학습을 안정화시키고, 수렴을 가속화하고 adaptive SGD의 일반화를 개선하는 데에 좋다. 이 논문에서는 그 매커니즘을 자세히 알아본다. adaptive lr은 초기 단계에 분산이 큰데, warmup이 분산 감소에 효과적이라는 이론을 제안하고 이를 검증한다. 또한 이 논문에서는 RAdam을 제시한다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;최근 adaptive lr이 빠른 수렴도때문에 많이 쓰이는데, 나쁜 local optima에 빠지지 않게 하고 학습을 안정화시키기 위해 warmup을 사용한다. 하지만 이 warmup에 대한 이론적인 토대는 충분하지 않기 때문에, 이를 스케줄링하는데 많은 노력이 들어간다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 warmup을 이론적, 실험적으로 해석한다. 정확하게는, adaptive learning이 제한된 training sample로 학습되기 때문에 초기에 큰 분산을 가지게 된다는 것을 보여주려 한다. 따라서 초기에 작은 lr을 이용해 warmup을 하면 더 작은 분산을 갖게되며 학습이 잘 된다는 것을 보여준다. 더해서, Adam의 변형인 RAdam을 제안하는데, 이는 Adam에서 adaptive lr의 분산 항을 수정한 것이다.&lt;/p&gt;

&lt;h2 id=&quot;2-preliminaries-and-motivations&quot;&gt;[2] Preliminaries and Motivations&lt;/h2&gt;

&lt;h3 id=&quot;generic-adaptive-methods&quot;&gt;Generic adaptive methods&lt;/h3&gt;

&lt;p&gt;모든 알고리즘은 $\phi$와 $\psi$의 선택에 따라 달라진다.&lt;/p&gt;

&lt;h3 id=&quot;learning-rate-warmup&quot;&gt;Learning rate warmup&lt;/h3&gt;

&lt;p&gt;warmup이 왜 잘되는지 알기 위해 gradient의 절댓값을 log scale로 그려보았는데, warmup이 없을 때에는 큰 gradient가 많았지만 있는 경우에는 작은 gradient가 많았다. 이 말은 처음 몇 단계에서는 나쁜 local optima에 빠진다는 것을 의미한다.&lt;/p&gt;

&lt;h2 id=&quot;3-variance-of-adaptive-rate&quot;&gt;[3] Variance of Adaptive Rate&lt;/h2&gt;

&lt;p&gt;“학습 초기에는 샘플이 별로 없어서 adaptive learning rate의 분산이 커져서 나쁜 local optima로 향한다” 가 이논문이 주장하는 가설이다.&lt;/p&gt;

&lt;h3 id=&quot;31-warmup-as-variance-reduction&quot;&gt;[3.1] Warmup as Variance Reduction&lt;/h3&gt;

&lt;p&gt;Adam-2k와 Adam-eps라는 Adam의 변형들이 실험에 사용되었는데, 이 논문에서는 실험을 위해 IWSLT’14 German to English 데이터셋을 사용했다. Adam-2k에서는 초기 2k 반복동안에 adaptive learning rate $\psi$만 업데이트되고 momentum $\phi$와 파라미터 $\theta$는 고정시켰는데, 이 방법은 vanilla Adam의 수렴 문제를 해결했고 초기 단계에서의 샘플의 부족이 gradient의 분포를 왜곡시킨다는 점을 알 수 있다. &lt;em&gt;이게 왜 샘플의 개수 때문이지..?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;또한, 이 논문에서는 adaptive learning rate의 분산을 작게 만듦으로써 이 수렴 문제를 해결할 수 있다는 점도 증명했다. Adam-eps에서는 adaptive lr의 분산을 작게 하기 위해 $\hat{\psi}(.)$에서의 $\epsilon$을 크게 설정했다. 이것은 또한 vanilla Adam의 수렴 문제를 해결했지만, 결과가 좀 나빴다. 이 논문에서는 $\epsilon$을 크게 설정하는 것이 bias를 야기해서 최적화 과정을 느리게 만들기 때문이라고 추측한다.&lt;/p&gt;

&lt;h3 id=&quot;32-analysis-of-adaptive-learning-rate-variance&quot;&gt;[3.2] Analysis of Adaptive Learning Rate Variance&lt;/h3&gt;

&lt;p&gt;[Thm1] 만약 $\psi^2(.)$이 $\text{Scaled-inv-}\chi^2(\rho, \frac{1}{\sigma^2})$을 따른다면, $\rho$가 증가하면 $\text{Var}(\psi(.))$은 감소한다. 이 이론은 초기 단계의 샘플 부족으로 안해 $\text{Var}(\psi(.))$가 커진다는 것을 보여준다. &lt;em&gt;이것도..왜 샘플의 개수 때문이지?&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-rectified-adaptive-learning-rate&quot;&gt;[4] Rectified Adaptive Learning Rate&lt;/h2&gt;

&lt;h3 id=&quot;41-estimate-of-rho&quot;&gt;[4.1] Estimate of $\rho$&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\frac{(1-\beta_2)\sum_{i=1}^t\beta_2^{t-i}g_i^2}{1-\beta_2^t})\approx p(\frac{\sum_{i=1}^{f(t, \beta_2)}g_{t+1-i}}{f(t,\beta_2)})&lt;/script&gt;

&lt;p&gt;로 주로 근사된다. $f(t, \beta_2)$는 SMA의 length인데, 이는 SMA와 EMA가 같은 무게중심을 갖게 해야 한다. 따라서 $f(t,\beta_2)=\frac{2}{1-\beta_2}-1-\frac{2t\beta_2^t}{1-\beta_2^t}$가 되어야 한다.&lt;/p&gt;

&lt;p&gt;또한 우리는 $f(t,\beta_2)$로 $\rho$를 근사할 수 있으므로 $f(t,\beta_2)=\rho_t$라고 쓸 것이고, $\frac{2}{1-\beta_2}-1=\rho_{\infty}$라고 쓸 것이다.&lt;/p&gt;

&lt;h3 id=&quot;42-variance-estimation-and-rectification&quot;&gt;[4.2] Variance Estimation and Rectification&lt;/h3&gt;

&lt;p&gt;adaptive learning rate $\psi(.)$가 일관되는 분산을 가지게 하기 위해 이 논문에서는 rectification을 이용했다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Var}[r_t\psi(g_1, \cdots, g_t)]=C_{\text{Var}}&lt;/script&gt;

&lt;p&gt;이때 $r_t=\sqrt{\frac{C_{\text{Var}}}{\text{Var}[\psi(g_1, \cdots, g_t)]}}$이고 $C_{\text{Var}}=\text{Var}[\psi(.)]|_  {\rho_t=\rho_{\infty}}$이다.&lt;/p&gt;

&lt;p&gt;$\text{Var}[\psi(.)]$는 수치적으로 stable하지 않으므로 rectified term을 1차 근사로 계산한다. 결론적으로 구한 $r_t$는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r_t=\sqrt{\frac{(\rho_t-4)(\rho_t-2)}{(\rho_\infty-4)(\rho_\infty-2)}}&lt;/script&gt;

&lt;p&gt;이다. 이것을 적용한 변형된 Adam을 RAdam이라고 한다.&lt;/p&gt;

&lt;h3 id=&quot;43-in-comparison-with-warmup&quot;&gt;[4.3] In Comparison with Warmup&lt;/h3&gt;

&lt;p&gt;$r_t$는 warmup과 비슷한 효과를 내며, warmup이 분산을 감소시키는 방식으로 작동된다는 것을 알았다. RAdam은 분산이 커질 때 adaptive lr을 비활성화시켜서 초기 불안정성을 막고, 하이퍼파라미터도 필요하지 않다.&lt;/p&gt;

&lt;h2 id=&quot;5-experiments&quot;&gt;[5] Experiments&lt;/h2&gt;

&lt;h3 id=&quot;51-comparing-to-vanilla-adam&quot;&gt;[5.1] Comparing to Vanilla Adam&lt;/h3&gt;

&lt;p&gt;adaptive learning rate은 초기에 큰 분산을 가지고 있다. 이를 보완하기 위해 이 논문에서는 RAdam을 만들었는데, 이는 Adam보다 결과가 좋을 뿐만 아니라 learning rate에도 민감하지 않다. (라는데 learning rate에 민감하지 않은지는 그래프를 봐도 뚜렷하게 나타나지는 않는다.)&lt;/p&gt;

&lt;p&gt;이 논문에서는 One billion word dataset (Language Modeling), CIFAR10, ImageNet (Image Classification)에 실험을 했는데, 이때의 결과로는 초기 분산 감소가 빠르고 정확도도 더 높은 결과를 가져오는데 좋은 역할을 했음을 알 수 있다. SGD와도 비교했는데, test accuracy는 SGD보다 좋지 않지만 train accuracy는 SGD보다 더 좋았다. (그런데 이말은 곧 overfitting이 더 된다는 말인 것 같다.)&lt;/p&gt;

&lt;p&gt;또, RAdam, vanilla Adam, Adam with warmup을 여러 범위의 lr로 비교한 결과, RAdam이 가장 민감하지 않았다고 하는데, 이 부분은 그래프에서 그리 뚜렷한 차이가 보이지는 않는다.&lt;/p&gt;

&lt;h3 id=&quot;52-comparing-to-heuristic-warmup&quot;&gt;[5.2] Comparing to Heuristic Warmup&lt;/h3&gt;

&lt;p&gt;IWSLT 데이터셋에 실험해본 결과, RAdam은 Adam에 warmup을 사용한 것과 비슷한 양상을 보인다. 또한 RAdam은 더 적은 하이퍼파라미터 튜닝을 요한다. 이 결과 또한 RAdam이 분산이 큰 상황을 없애기 때문이라는 주장에 보탬이 된다.&lt;/p&gt;

&lt;h3 id=&quot;53-simulated-verification&quot;&gt;[5.3] Simulated Verification&lt;/h3&gt;

&lt;p&gt;실험으로 1차 근사한 rectification term도 잘 근사되었다는 것을 확인했고, 그 결과 rectification 된 $\psi(.)$의 분산이 일정하다는 점도 확인했다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;이 논문에서는 optimizer로 Adam을 사용한다.&lt;/p&gt;
</description>
        <pubDate>Fri, 27 Dec 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/12/27/Adaptivelr.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/12/27/Adaptivelr.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Deep Residual Learning for Image Recognition</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;residual learning 이라는 framework을 만들었는데, 이 방법은 layer input에 대한 reference가 된다. 이 방법을 이용해서 더 깊은 네트워크를 더 쉽고 정확하게 학습시킬 수 있다. 여기다 앙상블을 쓴 결과는 ImageNet에서 3.57%의 에러를 갖는다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;네트워크의 깊이가 적당히 깊으면 학습이 잘되는데, 계속 깊게하면 성능이 저하된다. 이는 일반화에 관한 것이 아니고 train error 또한 크다. 네트워크를 더 깊게 만드기 위해 shortcut connection이라는 것을 만들었다. 만약 identity mapping이 최적이라면 이 네트워크는 0으로 학습된다.&lt;/p&gt;

&lt;h2 id=&quot;3-deep-residual-learning&quot;&gt;[3] Deep Residual Learning&lt;/h2&gt;

&lt;h3 id=&quot;31-residual-learning&quot;&gt;[3.1] Residual Learning&lt;/h3&gt;

&lt;p&gt;원래의 learning되는 함수가 $\mathcal{H}(x)$라고 하면, shortcut connection을 이용해 학습되는 함수는 $\mathcal{F}(x)+x$가 된다. 이렇게 학습되면 더이상 학습이 필요하지 않은 경우는 identity mapping이 되기 때문에 더 얕은 네트워크보다 성능이 저하되지 않는다. 이렇게 하면 새로 네트워크를 학습시키는 것보다 identity를 참조해서 복잡한 부분을 더 쉽게 찾을 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;32-identity-mapping-by-shortcuts&quot;&gt;[3.2] Identity Mapping by Shortcuts&lt;/h3&gt;

&lt;p&gt;building block은 다음과 같은 모양이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = \mathcal{F}(x, \{W_i\})+x&lt;/script&gt;

&lt;p&gt;plain network와 residual network는 같은 수의 파라미터, 깊이, 넓이, 계산복잡도를 갖는다. 만약 input과 output이 같은 차원을 갖지 않으면 그를 맞추기 위해 다음과 같이 설정한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = \mathcal{F}(x, \{W_i\})+W_sx&lt;/script&gt;

&lt;p&gt;그러나 identity mapping이 성능 저하를 줄이는 데에 좋으므로, 이런 형태는 차원 맞추기에만 사용된다. $\mathcal{F}$는 주로 그 안에 여러 층을 갖는다. 만약 한 층만 갖게 되면 선형 모양이 되기 때문에 plain network와 비교했을 때 별다른 이득을 얻지 못한다.&lt;/p&gt;

&lt;h3 id=&quot;33-network-architectures&quot;&gt;[3.3] Network Architectures&lt;/h3&gt;

&lt;p&gt;이 논문에서의 baseline은 VGGnet에서 영감을 받았다.&lt;/p&gt;

&lt;h2 id=&quot;4-experiments&quot;&gt;[4] Experiments&lt;/h2&gt;

&lt;h3 id=&quot;41-imagenet-classification&quot;&gt;[4.1] ImageNet Classification&lt;/h3&gt;

&lt;p&gt;34layer net 안에 18layer net의 모든 경우의 수가 들어갈 수 있음에도 불구하고 34layer net의 train error가 더 작았다. 따라서 degradation 현상을 목격했다고 할 수 있다. 이 논문에서는 실험에 BN을 사용했기 때문에 gradient vanishing 현상에 의해 나타났다고 보기 어렵다. &lt;em&gt;이 논문에서는 deep plain net이 수렴도가 매우 낮기 때문이라고 생각한다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ResNet에서는 더 깊은 네트워크가 train error가 더 작았고, 일반화도 더 잘 되었다. 또한 ResNet이 더 수렴속도가 빨랐다. 세 가지 구조가 있는데, (A) 차원을 늘리기 위해 zero-padding shortcut을 사용하고, 모든 shortcut은 parameter-free (B) 차원을 늘리기 위해 projection shortcut을 사용하고 다른 shortcut은 identity (C) 모든 shortcut은 projection. 그러나 이 결과들이 아주 조금의 차이만 있기 때문에, 이 논문에서는 projection shortcut이 residual learning에서 필수적인 요소는 아니라고 결론짓는다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 3층의 bottleneck 구조를 사용하는데, 이 구조는 2층의 구조와 시간복잡도는 비슷하고, convolution에서의 input과 output의 크기는 줄여준다. 만약 여기서 parameter-free가 아닌 projection으로 대체된다면, 시간복잡도가 배가 될 것이므로 identity shortcut이 더 효율적이다. 이 구조를 이용해 더 깊은 50layer, 101layer, 152layer를 만들어냈는데, 이들은 degradation 문제가 없으며, 더 깊을수록 결과가 더 좋았다.&lt;/p&gt;

&lt;h3 id=&quot;42-cifar-10-and-analysis&quot;&gt;[4.2] CIFAR-10 and Analysis&lt;/h3&gt;

&lt;p&gt;CIFAR-10에서는 warmup을 위해 초기 learning rate을 0.01로 두고 training error가 80%이하가 된 이후에 0.1로 높였고, 그 이후의 learning rate schedule은 이전과 같이 정했다. 이 논문에서는 layer마다 표준편차를 계산했는데, ResNet은 plain net보다 더 표준편차가 적은 것을 볼 수 있었다. 그리고 더 깊은 network일수록 표준편차가 더 작았다.&lt;/p&gt;

&lt;p&gt;그러나 1000layer가 넘는 network에서는 그보다 적은 layer의 network와 같은 train error를 가져도 test error가 더 높았다. &lt;em&gt;이 논문에서는 이가 overfitting 때문이라고 생각한다. 그러나 이 논문에서는 dropout같은 정규화 방법을 쓰지 않았기 때문에 이 방법들을 사용하면 결과가 더 나아질 것이라고 본다.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;10-crop test : 이미지의 각 코너와 center crop을 하고, flip된 이미지도 마찬가지로 처리하면 10개이다.&lt;br /&gt;
Pytorch에 ResNet이 구현되어 있는데, 이는 ImageNet을 위한 구조이고, CIFAR-10을 위한 구조는 다르다.&lt;/p&gt;
</description>
        <pubDate>Mon, 23 Dec 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/12/23/ResNet.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/12/23/ResNet.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
  </channel>
</rss>
